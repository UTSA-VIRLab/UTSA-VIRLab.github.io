<!DOCTYPE html>
<html lang="en" data-dark="false">
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!--
  put your analytics (e.g. Google Analytics) tracking code here
-->

  <!--
  put your search engine verification (e.g. Google Search Console) tag here
-->

  


























<meta name="viewport" content="width=device-width, initial-scale=1">

<title>Vision and Immersive Realities Lab</title>

<link rel="icon" href="/images/icon.png">

<meta name="title" content="">
<meta name="description" content="CS@UTSA. An easy-to-use, flexible website template for labs, with automatic citations, GitHub tag imports, pre-built components, and more.">

<meta property="og:title" content="">
<meta property="og:site_title" content="Vision and Immersive Realities Lab">
<meta property="og:description" content="CS@UTSA. An easy-to-use, flexible website template for labs, with automatic citations, GitHub tag imports, pre-built components, and more.">
<meta property="og:url" content="https://utsa-virlab.github.io">
<meta property="og:image" content="/images/share.jpg">
<meta property="og:locale" content="en_US">

<meta property="twitter:title" content="">
<meta property="twitter:description" content="CS@UTSA. An easy-to-use, flexible website template for labs, with automatic citations, GitHub tag imports, pre-built components, and more.">
<meta property="twitter:url" content="https://utsa-virlab.github.io">
<meta property="twitter:card" content="summary_large_image">
<meta property="twitter:image" content="/images/share.jpg">


  <meta property="og:type" content="website">


<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "WebSite",
    
    "name": "",
    "description": "CS@UTSA. An easy-to-use, flexible website template for labs, with automatic citations, GitHub tag imports, pre-built components, and more.",
    "headline": "",
    "publisher": {
      "@type": "Organization",
      "logo": { "@type": "ImageObject", "url": "/images/icon.png" }
    },
    "url": "https://utsa-virlab.github.io"
  }
</script>

<link rel="alternate" type="application/rss+xml" href="https://utsa-virlab.github.io/feed.xml">

  <!-- Google Fonts -->
<!-- automatically get url from fonts used in theme file -->

<link rel="preconnect" href="https://fonts.gstatic.com">
<link href="https://fonts.googleapis.com/css2?display=swap&&family=Barlow:ital,wght@0,200;0,400;0,500;0,600;1,200;1,400;1,500;1,600&amp;family=Roboto+Mono:ital,wght@0,200;0,400;0,500;0,600;1,200;1,400;1,500;1,600" rel="stylesheet">

<!-- Font Awesome icons (load asynchronously due to size) -->

<link href="https://use.fontawesome.com/releases/v6.5.0/css/all.css" rel="stylesheet" media="none" onload="this.removeAttribute('media'); this.onload = null;">
<noscript>
  <link href="https://use.fontawesome.com/releases/v6.5.0/css/all.css" rel="stylesheet">
</noscript>

  <!-- third party styles -->
<!-- https://stylishthemes.github.io/Syntax-Themes/pygments/ -->
<link href="https://cdn.jsdelivr.net/gh/StylishThemes/Syntax-Themes/pygments/css-github/pygments-tomorrow-night-eighties.css" rel="stylesheet">

<!-- include all sass in styles folder -->


  
    <link href="/_styles/-theme.css" rel="stylesheet">
  

  
    <link href="/_styles/alert.css" rel="stylesheet">
  

  
    <link href="/_styles/all.css" rel="stylesheet">
  

  
    <link href="/_styles/anchor.css" rel="stylesheet">
  

  
    <link href="/_styles/background.css" rel="stylesheet">
  

  
    <link href="/_styles/body.css" rel="stylesheet">
  

  
    <link href="/_styles/bold.css" rel="stylesheet">
  

  
    <link href="/_styles/button.css" rel="stylesheet">
  

  
    <link href="/_styles/card.css" rel="stylesheet">
  

  
    <link href="/_styles/checkbox.css" rel="stylesheet">
  

  
    <link href="/_styles/citation.css" rel="stylesheet">
  

  
    <link href="/_styles/code.css" rel="stylesheet">
  

  
    <link href="/_styles/cols.css" rel="stylesheet">
  

  
    <link href="/_styles/dark-toggle.css" rel="stylesheet">
  

  
    <link href="/_styles/details.css" rel="stylesheet">
  

  
    <link href="/_styles/feature.css" rel="stylesheet">
  

  
    <link href="/_styles/figure.css" rel="stylesheet">
  

  
    <link href="/_styles/float.css" rel="stylesheet">
  

  
    <link href="/_styles/font.css" rel="stylesheet">
  

  
    <link href="/_styles/footer.css" rel="stylesheet">
  

  
    <link href="/_styles/form.css" rel="stylesheet">
  

  
    <link href="/_styles/grid.css" rel="stylesheet">
  

  
    <link href="/_styles/header.css" rel="stylesheet">
  

  
    <link href="/_styles/heading.css" rel="stylesheet">
  

  
    <link href="/_styles/highlight.css" rel="stylesheet">
  

  
    <link href="/_styles/home.css" rel="stylesheet">
  

  
    <link href="/_styles/icon.css" rel="stylesheet">
  

  
    <link href="/_styles/image.css" rel="stylesheet">
  

  
    <link href="/_styles/link.css" rel="stylesheet">
  

  
    <link href="/_styles/list.css" rel="stylesheet">
  

  
    <link href="/_styles/main.css" rel="stylesheet">
  

  
    <link href="/_styles/member.css" rel="stylesheet">
  

  
    <link href="/_styles/paragraph.css" rel="stylesheet">
  

  
    <link href="/_styles/portrait.css" rel="stylesheet">
  

  
    <link href="/_styles/post-excerpt.css" rel="stylesheet">
  

  
    <link href="/_styles/post-info.css" rel="stylesheet">
  

  
    <link href="/_styles/post-nav.css" rel="stylesheet">
  

  
    <link href="/_styles/project.css" rel="stylesheet">
  

  
    <link href="/_styles/quote.css" rel="stylesheet">
  

  
    <link href="/_styles/rule.css" rel="stylesheet">
  

  
    <link href="/_styles/search-box.css" rel="stylesheet">
  

  
    <link href="/_styles/search-info.css" rel="stylesheet">
  

  
    <link href="/_styles/section.css" rel="stylesheet">
  

  
    <link href="/_styles/sponsors.css" rel="stylesheet">
  

  
    <link href="/_styles/table.css" rel="stylesheet">
  

  
    <link href="/_styles/tags.css" rel="stylesheet">
  

  
    <link href="/_styles/textbox.css" rel="stylesheet">
  

  
    <link href="/_styles/tooltip.css" rel="stylesheet">
  

  
    <link href="/_styles/util.css" rel="stylesheet">
  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  


<!-- include all css in styles folder -->



  <!-- third party scripts -->
<script src="https://unpkg.com/@popperjs/core@2" defer></script>
<script src="https://unpkg.com/tippy.js@6" defer></script>
<script src="https://unpkg.com/mark.js@8" defer></script>

<!-- include all js in scripts folder -->


  <script src="/_scripts/anchors.js"></script>

  <script src="/_scripts/dark-mode.js"></script>

  <script src="/_scripts/fetch-tags.js"></script>

  <script src="/_scripts/home.js"></script>

  <script src="/_scripts/projects.js"></script>

  <script src="/_scripts/publications.js"></script>

  <script src="/_scripts/search.js"></script>

  <script src="/_scripts/site-search.js"></script>

  <script src="/_scripts/table-wrap.js"></script>

  <script src="/_scripts/tooltip.js"></script>


</head>

  <body>
    







<header class="background" style="--image: url('/images/background.jpg')" data-dark="true">
  <a href="/" class="home">
    
      <span class="logo">
        
          <img src="/images/logo.png" alt="logo">
        
      </span>
    
    
      <span class="title-text" data-tooltip="Home">
        
          <span class="title">Vision and Immersive Realities Lab</span>
        
        
          <span class="subtitle">CS@UTSA</span>
        
      </span>
    
  </a>

  <input class="nav-toggle" type="checkbox" aria-label="show/hide nav">

  <nav>
    
    
      
        <a href="/" data-tooltip="Back to the lab’s homepage">
          Home
        </a>
      
    
      
        <a href="/projects/" data-tooltip="Projects and funding">
          Projects
        </a>
      
    
      
        <a href="/research/" data-tooltip="Published works">
          Publications
        </a>
      
    
      
        <a href="/team/" data-tooltip="About our team">
          People
        </a>
      
    
      
        <a href="/sponsors/" data-tooltip="Sponsors &amp; Partners">
          Sponsors
        </a>
      
    
  </nav>
</header>

    <main>
      <!--
  modify main content of page:
  - add section breaks
  - attach section properties
  - filter out blank sections
-->






  
  
  

  <section class="background" data-size="page">
    <div class="project-detail">
    <!-- 1) Project header -->
    <div id="project-header"></div>

    <!-- 2) Related publications -->
    <section id="related-pubs" style="display:none;">
        <h2>Related Publications</h2>
        <div id="pubs-list"></div>
    </section>

    <!-- 3) Global BibTeX Modal (exactly the same as on your publications page) -->
    <div id="globalBibtexModal" class="modal">
        <div class="modal-content">
            <span class="close">×</span>
            <pre id="globalBibtexContent"></pre>
            <button class="copy-bibtex" data-target="globalBibtexContent">Copy</button>
        </div>
    </div>
</div>

<!-- Embed your JSON data -->
<script id="projects-data" type="application/json">
    [{"title":"Identifying Bias in STEM Workplaces Towards Persons with Disabilities","id":"seed-bias","agency":"UTSA VPR Office - SRA RIG","award":"$20,000","investigator":"John Quarles, Kathy Ewoldt, Kevin Desai","role":"Co-PI (33%)","timeline":"12/2024 - 07/2025","group":"past","image":"images/photo.jpg","description":null,"tags":["research"]},{"title":"HCC: Medium: Adaptive Auditory Feedback to Improve Balance in Virtual Reality at Home","id":"nsf-medium-audio","agency":"NSF - (CISE) Core Programs - Medium","award":"$1,200,000","investigator":"John Quarles, Kevin Desai, Alberto Cordova","role":"Co-PI (33%)","timeline":"10/2024 - 09/2027","group":"current","image":"images/photo.jpg","link":"https://www.nsf.gov/awardsearch/showAward?AWD_ID=2403411","description":"Consumer-level virtual reality goggles are not accessible for many persons with balance impairments, such as elderly persons, persons with multiple sclerosis, Parkinson's, or diabetes. Currently, people with balance impairments cannot benefit from many immersive virtual reality benefits, such as education, physical fitness, rehabilitation, and entertainment. The team's previous work studied how audio, visuals, and vibrations can improve balance while in virtual reality in controlled laboratory settings with simplified virtual reality environments, where they found that audio was the most effective. This project aims to expand the research beyond laboratory settings because typical use of virtual reality occurs at home with complex virtual reality environments with much more intense audio, visual, and vibration stimulation. To address the additional complexities of being outside the lab with much stronger stimulation, the team will develop specialized audio intended to improve balance in any virtual environment. If virtual reality imbalance issues can be resolved, persons with balance impairments can more readily benefit from virtual reality. This project investigates approaches to enable adaptive auditory feedback techniques to improve balance in virtual reality use at home, which includes commercial virtual reality experiences with strong stimuli. Based on the team's preliminary studies, the central hypothesis is that adaptive auditory feedback can improve balance during virtual reality use at home more effectively than the current state of the art, which uses a static, 'one size fits all' approach to feedback. The work will seek the following novel contributions: 1) adaptive auditory feedback techniques to improve balance in VR; 2) automatic, real-time balance prediction with low-cost sensors, some of which are already integrated into commercial virtual reality systems; 3) controlled laboratory and at-home studies, providing generalizable understanding and computational models of balance in virtual reality in the presence of strong stimuli; 4) insight into the potentially lasting effects of virtual reality-based auditory feedback on balance after virtual reality exposure. Ultimately, this project will result in datasets and open-source tools that will make virtual reality more accessible for persons with balance impairments, which could improve their quality of life.","tags":["research"]},{"title":"Exploring Online Learning in VR-Supported STEM Laboratories","id":"seed-chem-labs","agency":"UTSA VPR Office - UTSA-ITESM Seed Funding Program","award":"$40,000","investigator":"Kevin Desai, John Quarles","role":"Co-PI (50%)","timeline":"06/2024 - 08/2025","group":"past","image":"images/photo.jpg","link":"https://www.utsa.edu/today/2024/09/story/utsa-tec-de-monterrey-tap-virtual-reality-tools.html","description":null,"tags":["research"]},{"title":"HCC: Small: Making Virtual Reality Safe","id":"nsf-small-safety","agency":"NSF - (CISE) Core Programs","award":"$600,000","investigator":"John Quarles, Kevin Desai","role":"Co-PI (50%)","timeline":"01/2024 - 12/2026","group":"current","image":"images/photo.jpg","link":"https://www.nsf.gov/awardsearch/showAward?AWD_ID=2316240","description":"Although consumer-level virtual reality head-mounted displays have become affordable enough for a broad user base to purchase, there are still serious concerns about safety. Head-mounted displays block out the surrounding real world, which can hide obstacles, such as tables, pets, walls, or other potential collision hazards. Current approaches to avoiding collisions depend on the user to define play area boundaries; this process is subject to user error and thus can lead to injury. Moreover, current approaches are ineffective for games that require fast motions, as the systems may not react in time to prevent injury. To address these problems, the investigators will create camera-based methods for detecting potential collisions in real time and evaluate feedback techniques to reduce the incidence of injury when using virtual reality headsets. This approach has the potential to make the use of virtual reality much safer in real-world environments. The objective of this project is to create and evaluate reconstruction, segmentation, and motion-prediction techniques to inform obstacle avoidance feedback and reduce the incidence of injury to people using virtual reality head mounted displays. Providing feedback tailored to the specific locations of the user's body that are in proximity to real obstacles will, ideally, reduce user collisions with real-world objects compared to prior approaches. Specifically, the team will 1) determine the best approaches to real obstacle detection and segmentation, 2) investigate the efficacy of full body motion prediction approaches, 3) ascertain the optimal modality and locations of real obstacle alerts to maximize presence while minimizing collisions, and 4) evaluate the longitudinal impact of real obstacle alert systems in virtual reality systems. Ultimately, this research will result in human motion datasets that can be used for future motion prediction in virtual reality research, as well as open-source plugins that will make current virtual reality experiences safer by reducing injuries.","tags":["research"]},{"title":"HCI in Motion -- Using EEG, Eye Tracking, and Body Sensing for Attention-Aware Mobile Mixed Reality","id":"nsf-medium-motion","agency":"NSF - (CISE) Core Programs","award":"$457,105","investigator":"John Quarles, Kevin Desai","role":"Co-PI (50%)","timeline":"09/2022 - 08/2026","group":"current","image":"images/photo.jpg","link":"https://www.nsf.gov/awardsearch/showAward?AWD_ID=2211785","description":"Mobile, wireless, headsets for virtual and augmented reality, such as the Meta Quest 2 and Microsoft HoloLens-2, are becoming more widely used in many applications beyond video games, such as training, construction, and medicine. However, wearing these head-worn goggles while walking can make some people feel sick or distracted, which has even led to injury in some cases. This effect is similar to texting while walking, but potentially worse because a person's entire periphery can be filled with distracting media elements. While previous research has investigated these issues when users are standing still or seated, it is unclear how problems unfold and how they can be prevented while users are in motion. Specifically, this project will investigate how and why virtual and augmented reality headsets affect attention and feelings of sickness. First, this work will record data, such as heart rate, brain waves, and the direction users are turning their eyes to, while they are wearing virtual and augmented reality headsets and walking. Secondly, this project will develop ways to reduce sickness and distraction while walking with virtual and augmented reality headsets. This work will improve the safety of mobile virtual and augmented reality headsets, products that virtually all big technology companies today heavily invest in as possible companions or replacements to smartphones. This project will be introduced in courses and research mentorship projects at The University of Texas at San Antonio and the University of California at Santa Barbara, to advance research training of both undergraduate and graduate students. Considering that both universities and research teams have a history of supporting many underrepresented minority students, it is expected that the educational value of this project will be high, especially in terms of recruiting and mentoring women and underrepresented minority students. There is an increasing prevalence of mobile, immersive interfaces (e.g., mobile Virtual Reality(VR) / Augmented Reality (AR)) that may affect users' cognitive capacities and situational awareness, potentially leading to physical harm (e.g., impaired task performance, tripping over physical obstacles in VR, unsafe street crossings while seeing advertisements in AR). The landscape of human-computer interaction has expanded from fairly well standardized stationary office configurations to more varied mobile and immersive settings involving active body movements (mobile and situated computing, AR, mobile VR) and simulated first-person perspective changes and motion experiences (immersive computing). To make matters worse, compared to more standardized platforms such as desktop and laptop UIs, tablet and smartphone interfaces, individual differences among users have a much bigger usability impact in context-driven surround-focus usage scenarios found in mobile AR/VR. For example, motion sickness (i.e., cybersickness) in VR is known to inflict symptoms of widely varying severity, depending on the individual user. One serious consequence is that interaction designers have difficulties providing engaging general experiences that are universally usable by a wide variety of users. Despite the increasing prevalence of immersive technologies and their pitfalls, the precise cognitive and physiological mechanisms at play when 'computing in motion' are not well understood. This work is aimed at filling this knowledge gap. The specific objectives are: 1) to assess the cognitive effects of interacting with mobile AR/VR while users are walking, 2) to provide automated tools to effectively reduce the cognitive demand of mobile AR/VR, and 3) to make mobile AR/VR safer and more usable. Based on preliminary data, the central hypothesis is that through multi-modal sensing combined with machine learning approaches, mobile AR/VR applications can learn the characteristics of user behavior and provide real time adaptations that will reduce user error, increase ease of use, improve task performance, and reduce the impact of physical hazards. This work will improve the safety of mobile AR and mobile VR, paradigms that virtually all big technology companies today heavily invest in as a possible follow-up paradigm to the smartphone platform. Educational impact will occur through incorporation of research outcomes into undergraduate and graduate courses offered at The University of Texas at San Antonio and the University of California at Santa Barbara, and research training and mentorship opportunities for both undergraduate and graduate students. The courses include Machine Learning, Deep Learning for Visual Computing, Human-Computer Interaction, and Mobile Application Programming. Because our project integrates a topic of high social impact with cutting edge machine learning and human-computer interaction research along with proven successful mentorship strategies, the educational impacts of the project will be high, especially in terms of recruiting and mentoring women and underrepresented minority students.","tags":["research"]},{"title":"CRII: HCC: 3D Hand & Full-Body Pose Estimation in Telehealth for Children with Autism","id":"nsf-crii","agency":"NSF - (CISE) Research Initiation Initiative (CRII)","award":"$174,368","investigator":"Kevin Desai","role":"PI (100%)","timeline":"06/2022 - 12/2025","group":"current","image":"images/photo.jpg","link":"https://www.nsf.gov/awardsearch/showAward?AWD_ID=2153249","description":"The overall objective of this project is to provide efficient full-body interaction in virtual reality systems that do not use head-mounted displays. This project aims to create accurate and real-time 3D hand and body pose estimation, in the highly significant application area of children with autism. A novel synthetic hand data generation framework will generate 3D hand poses with increased diversity in terms of hand distance from camera, hand size, camera viewpoint, occlusion, background, and skin color. The outcome will be a novel 3D synthetic hand dataset consisting of realistic and kinematically accurate hand models with articulated poses that will advance current and future research endeavors in 3D hand pose estimation research. The project will advance the state-of-the-art in 3D body pose estimation for humans present further away from the camera at room-scale distances. The synthetic dataset, algorithms, and programming libraries will be made publicly available for wide-spread adoption, thereby advancing pose estimation research. This research will have broad societal impact because it will improve the usability and interaction in human centered telehealth applications, initially helping with the applied behavior analysis for children with autism. Existing systems that employ head-mounted displays or wearable sensors for tracking the user's hand and body movements are not suitable for children with autism, and have disadvantages in many other application areas. Therefore, by enabling 3D hand and full-body pose estimation, this project will advance a plethora of 3D immersive applications such as education, virtual STEM laboratories, tele-rehabilitation, tele-operation, military training, entertainment, and communication. The need for real-time, remote and interactive human motion sensing exists now more than ever, considering the increase in virtual activities because of the recent pandemic.","tags":["research"]},{"title":"Enhancing collaboration and embodied learning in online virtual reality STEM laboratories through high-fidelity interactive avatars","id":"nsf-ritel","agency":"NSF - Research on Innovative Technologies for Enhanced Learning (RITEL)","award":"$600,000","investigator":"Kevin Desai, John Quarles","role":"PI (50%)","timeline":"09/2025 - 08/2028","group":"current","image":"images/photo.jpg","link":"https://www.nsf.gov/awardsearch/showAward?AWD_ID=2506911","description":"The shift to online and hybrid learning has exposed critical limitations in existing virtual STEM laboratories, particularly their inability to replicate the hands-on, collaborative experiences essential for deep science learning. Traditional labs provide embodied and communicative interactions that are often absent in current online tools, especially in experiments involving delicate, dangerous, or spatially complex procedures. This project will address that gap by developing immersive virtual reality (VR) laboratory environments that feature realistic avatars and tactile feedback, enabling collaborative learning that mirrors the cognitive and social dynamics of physical labs. By advancing access to high-quality science education, the project supports national goals to enhance health, prosperity, and workforce development. Approximately 500 students and 50 teachers will be directly impacted through outreach and implementation efforts with the aim of equipping learners with the skills necessary for future scientific and technical careers and to increase the overall STEM workforce. <br><br>The project will investigate how immersive VR laboratories, enhanced with high-fidelity avatars and vibrotactile interfaces, support collaborative science learning. A key research objective will be to examine how variations in avatar visual realism and tactile interactivity influence conceptual understanding, procedural competence, emotional engagement, and collaboration. The central hypothesis is that increased embodiment and realism will improve both individual and team-based learning outcomes. Technological development will focus on reconstructing avatars with full-body fidelity and embedding sensory feedback systems that simulate real-world lab interactions. The research will include controlled user studies in chemistry, comparing learning and collaboration outcomes in VR labs versus traditional in-person labs. Data collection will include quantitative measures such as pre/post-tests and interaction performance metrics, alongside qualitative observations, interviews, and surveys. Analysis will involve both statistical and thematic techniques to capture the cognitive, affective, and collaborative impacts of immersive learning. The project will culminate in the release of an open-source VR lab platform and accompanying dataset, enabling broader access and seeding future innovations in collaborative online STEM education.","tags":["research"]},{"title":"LIGHT-SEAL: Hardware Trojan Aware Photonic Transformer for Secured Generative AI","id":"seed-photonic","agency":"UTSA SDS Collaborative Seed Funding Grant","award":"$35,000","investigator":"Dharnidhar Dang, Kevin Desai","role":"Co-PI (50%)","timeline":"07/2024 - 12/2024","group":"past","image":"images/photo.jpg","link":"https://sds.utsa.edu//news/2024/10/seedgrant.html","description":null,"tags":["research"]},{"title":"Towards Personalized Virtual Reality Interventions for Rehabilitation of Persons with Disabilities","id":"seed-mac","agency":"UTSA VPR Office - MAC-UTSA Seed Funding Program","award":"$25,000","investigator":"John Quarles, Kevin Desai, Alberto Cordova","role":"Co-PI (33%)","timeline":"10/2023 - 06/2024","group":"past","image":"images/photo.jpg","link":"https://www.utsa.edu/today/2024/06/story/UTSA-morgans-MAC-showcase-impactful-disability-research.html","description":null,"tags":["research"]},{"title":"Electronic Health Record Big Data and Radiomic Analytics for Precision Medicine Approach to Long-COVID","id":"sappt","subtitle":"a subtitle","agency":"San Antonio Partnership for Precision Therapeutics","award":"$50,000","investigator":"Dhireesha Kudithipudi, Kevin Desai, Anandi Dutta","role":"Co-PI (25%)","timeline":"08/2022 - 02/2024","group":"past","image":"images/photo.jpg","description":null,"tags":["research"]},{"title":"A step towards smart and connected health in behavior analysis","id":"seed-great-sleep","agency":"UTSA VPR Office - GREAT 2021","award":"$20,000","investigator":"Leslie Neely, Paul Rad, Qian Chen, Kevin Desai","role":"Co-PI (25%)","timeline":"10/2021 - 07/2022","group":"past","image":"images/photo.jpg","link":"https://www.utsa.edu/today/2021/08/story/knowledge-enterprise-funds-innovative-new-research-projects.html#:~:text=A%20step%20towards%20smart%20and%20connected%20health%20in%20behavior%20analysis","description":null,"tags":["research"]},{"title":"Project Lovelace 2.0: Advancing Women in AI Career Pathways","id":"xilinx","agency":"Xilinx Inc. WIT University Grants 2021","award":"$30,000","investigator":"Dhireesha Kudithipudi, Amina Qutub, Kevin Desai","role":"Co-PI (33%)","timeline":"09/2021 - 08/2022","group":"past","image":"images/photo.jpg","link":"https://ai.utsa.edu/xilinxfellows/","description":null,"tags":["instructional"]},{"title":"Broadening Participation in Computer Science","id":"","agency":"Northeastern University - Center for Inclusive Computing","award":"$499,801","role":"Co-PI (10%)","investigator":"Jianwei Niu, John Heaps, Amanda Fernandez, Mitra Bokaei Hosseini, Kevin Desai, Rocky Slavin","timeline":"01/2024 - 12/2025","group":"current","image":"images/photo.jpg","link":"https://sciences.utsa.edu/spotlight-news/2024/grant-to-prepare-students-for-computer-science-careers.html","description":null,"tags":["instructional"]},{"title":"MATCH: MATRIX AI/ML Concierge for Healthcare","id":"","agency":"NIH (UNT Passthrough)","award":"$500,000","role":"Co-PI (10%)","investigator":"Dhireesha Kudithipudi, Amina Qutub, Mark Goldberg, Ambika Mathur, Kevin Desai, Panagiotis Markopoulos, Erica Sosa","timeline":"10/2024 - 08/2025","group":"past","image":"images/photo.jpg","link":"https://healthyhuman.tech/","description":null,"tags":["service"]},{"title":"M-POWER: MATRIX-Provided AI/ML Open-Source Resource Center for Behavioral Health EmpoWERment","id":"","agency":"NIH (UNT Passthrough)","award":"$500,000","role":"Co-PI (10%)","investigator":"Dhireesha Kudithipudi, Amina Qutub, Mark Goldberg, Ambika Mathur, Kevin Desai, Panagiotis Markopoulos, Anandi Dutta, Erica Sosa","timeline":"10/2023 - 07/2024","group":"past","image":"images/photo.jpg","link":"https://ai.utsa.edu/mpower/","description":null,"tags":["service"]}]
</script>
<script id="citations-data" type="application/json">
    [{"id":"doi:10.1101/2025.06.19.660613","title":"Scalable data harmonization for single-cell image-based profiling with CytoTable","authors":["Dave Bunten","Jenna Tomkinson","Erik Serrano","Michael J. Lippincott","Kenneth I. Brewer","Vince Rubinetti","Faisal Alquaddoomi","Gregory P. Way"],"publisher":"Cold Spring Harbor Laboratory","date":"2025-06-25","link":"https://doi.org/g9rfr3","orcid":"0000-0002-4655-3773","plugin":"orcid.py","file":"orcid.yaml"},{"id":"doi:10.1093/nar/gkad1082","title":"The Monarch Initiative in 2024: an analytic platform integrating phenotypes, genes and diseases across species","authors":["Tim E Putman","Kevin Schaper","Nicolas Matentzoglu","Vincent P Rubinetti","Faisal S Alquaddoomi","Corey Cox","J Harry Caufield","Glass Elsarboukh","Sarah Gehrke","Harshad Hegde","Justin T Reese","Ian Braun","Richard M Bruskiewich","Luca Cappelletti","Seth Carbon","Anita R Caron","Lauren E Chan","Christopher G Chute","Katherina G Cortes","Vinícius De Souza","Tommaso Fontana","Nomi L Harris","Emily L Hartley","Eric Hurwitz","Julius O B Jacobsen","Madan Krishnamurthy","Bryan J Laraway","James A McLaughlin","Julie A McMurry","Sierra A T Moxon","Kathleen R Mullen","Shawn T O’Neil","Kent A Shefchek","Ray Stefancsik","Sabrina Toro","Nicole A Vasilevsky","Ramona L Walls","Patricia L Whetzel","David Osumi-Sutherland","Damian Smedley","Peter N Robinson","Christopher J Mungall","Melissa A Haendel","Monica C Munoz-Torres"],"publisher":"Nucleic Acids Research","date":"2023-11-24","link":"https://doi.org/gs6kmr","orcid":"0000-0002-4655-3773","plugin":"orcid.py","file":"orcid.yaml"},{"id":"doi:10.1101/2023.10.11.560955","title":"Integration of 168,000 samples reveals global patterns of the human gut microbiome","authors":["Richard J. Abdill","Samantha P. Graham","Vincent Rubinetti","Frank W. Albert","Casey S. Greene","Sean Davis","Ran Blekhman"],"publisher":"Cold Spring Harbor Laboratory","date":"2023-10-11","link":"https://doi.org/gsvf5z","orcid":"0000-0002-4655-3773","plugin":"orcid.py","file":"orcid.yaml"},{"id":"doi:10.1093/nar/gkad289","title":"MyGeneset.info: an interactive and programmatic platform for community-curated and user-created collections of genes","authors":["Ricardo Avila","Vincent Rubinetti","Xinghua Zhou","Dongbo Hu","Zhongchao Qian","Marco Alvarado Cano","Everaldo Rodolpho","Ginger Tsueng","Casey Greene","Chunlei Wu"],"publisher":"Nucleic Acids Research","date":"2023-04-18","link":"https://doi.org/gr5hb5","orcid":"0000-0002-4655-3773","plugin":"orcid.py","file":"orcid.yaml"},{"id":"doi:10.1101/2023.01.05.522941","title":"Hetnet connectivity search provides rapid insights into how two biomedical entities are related","authors":["Daniel S. Himmelstein","Michael Zietz","Vincent Rubinetti","Kyle Kloster","Benjamin J. Heil","Faisal Alquaddoomi","Dongbo Hu","David N. Nicholson","Yun Hao","Blair D. Sullivan","Michael W. Nagle","Casey S. Greene"],"publisher":"Cold Spring Harbor Laboratory","date":"2023-01-07","link":"https://doi.org/grmcb9","orcid":"0000-0002-4655-3773","plugin":"orcid.py","file":"orcid.yaml"},{"id":"doi:10.1093/gigascience/giad047","title":"Hetnet connectivity search provides rapid insights into how biomedical entities are related","authors":["Daniel S Himmelstein","Michael Zietz","Vincent Rubinetti","Kyle Kloster","Benjamin J Heil","Faisal Alquaddoomi","Dongbo Hu","David N Nicholson","Yun Hao","Blair D Sullivan","Michael W Nagle","Casey S Greene"],"publisher":"GigaScience","date":"2022-12-28","link":"https://doi.org/gsd85n","orcid":"0000-0002-4655-3773","plugin":"orcid.py","file":"orcid.yaml"},{"id":"doi:10.1101/2022.02.18.461833","title":"MolEvolvR: A web-app for characterizing proteins using molecular evolution and phylogeny","authors":["Jacob D Krol","Joseph T Burke","Samuel Z Chen","Lo M Sosinski","Faisal S Alquaddoomi","Evan P Brenner","Ethan P Wolfe","Vincent P Rubinetti","Shaddai Amolitos","Kellen M Reason","John B Johnston","Janani Ravi"],"publisher":"Cold Spring Harbor Laboratory","date":"2022-02-22","link":"https://doi.org/gstx7j","orcid":"0000-0002-4655-3773","plugin":"orcid.py","file":"orcid.yaml"},{"id":"doi:10.1186/s13059-020-02021-3","title":"Compressing gene expression data using multiple latent space dimensionalities learns complementary biological representations","authors":["Gregory P. Way","Michael Zietz","Vincent Rubinetti","Daniel S. Himmelstein","Casey S. Greene"],"publisher":"Genome Biology","date":"2020-05-11","link":"https://doi.org/gg2mjh","orcid":"0000-0002-4655-3773","plugin":"orcid.py","file":"orcid.yaml"},{"id":"doi:10.1371/journal.pcbi.1007128","title":"Open collaborative writing with Manubot","authors":["Daniel S. Himmelstein","Vincent Rubinetti","David R. Slochower","Dongbo Hu","Venkat S. Malladi","Casey S. Greene","Anthony Gitter"],"publisher":"PLOS Computational Biology","date":"2020-12-04","link":"https://doi.org/c7np","orcid":"0000-0002-4655-3773","plugin":"sources.py","file":"sources.yaml","type":"paper","description":"Lorem ipsum _dolor_ **sit amet**, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.","image":"https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=info:doi/10.1371/journal.pcbi.1007128.g001&rev=2","buttons":[{"type":"manubot","link":"https://greenelab.github.io/meta-review/","icon":"manubot.svg","text":"Manubot","tooltip":"Manubot"},{"type":"source","text":"Manuscript Source","link":"https://github.com/greenelab/meta-review","icon":"fa-solid fa-code","tooltip":"Source code"},{"type":"website","link":"http://manubot.org/","icon":"fa-solid fa-globe","text":"Website","tooltip":"Website"}],"tags":["open science","collaboration"],"repo":"greenelab/meta-review"},{"id":"doi:10.1101/573782","title":"Sequential compression of gene expression across dimensionalities and methods reveals no single best method or dimensionality","authors":["Gregory P. Way","Michael Zietz","Vincent Rubinetti","Daniel S. Himmelstein","Casey S. Greene"],"publisher":"Cold Spring Harbor Laboratory","date":"2019-03-11","link":"https://doi.org/gfxjxf","orcid":"0000-0002-4655-3773","plugin":"orcid.py","file":"orcid.yaml"},{"id":"doi:10.1016/j.csbj.2020.05.017","title":"Constructing knowledge graphs and their biomedical applications","authors":["David N. Nicholson","Casey S. Greene"],"publisher":"Computational and Structural Biotechnology Journal","date":"2020-01-01","link":"https://doi.org/gg7m48","image":"https://ars.els-cdn.com/content/image/1-s2.0-S2001037020302804-gr1.jpg","plugin":"sources.py","file":"sources.yaml"},{"id":"doi:10.7554/eLife.32822","title":"Sci-Hub provides access to nearly all scholarly literature","authors":["Daniel S Himmelstein","Ariel Rodriguez Romero","Jacob G Levernier","Thomas Anthony Munro","Stephen Reid McLaughlin","Bastian Greshake Tzovaras","Casey S Greene"],"publisher":"eLife","date":"2018-03-01","link":"https://doi.org/ckcj","image":"https://iiif.elifesciences.org/lax:32822%2Felife-32822-fig8-v3.tif/full/863,/0/default.webp","plugin":"sources.py","file":"sources.yaml"}]
</script>

<script>
    // -- Build page & inject "Show BibTeX" buttons --
    document.addEventListener("DOMContentLoaded", () => {
        const projects  = JSON.parse(document.getElementById("projects-data").textContent);
        const citations = JSON.parse(document.getElementById("citations-data").textContent);

        // lookup project
        const projId    = new URLSearchParams(window.location.search).get("id");
        const proj      = projects.find(p => p.id === projId);
        const headerEl  = document.getElementById("project-header");
        const relatedEl = document.getElementById("related-pubs");
        const listEl    = document.getElementById("pubs-list");

        if (!proj) {
            headerEl.innerHTML = "<p><em>Project not found.</em></p>";
            return;
        }

        // — Render project header
        let meta = "<dl class='project-meta'>";
        ["agency","award","investigator","role","timeline"].forEach(key => {
            const label = key.charAt(0).toUpperCase() + key.slice(1);
            meta += `<dt>${label}:</dt><dd>${proj[key]||"&mdash;"}</dd>`;
        });
        if (proj.link) {
            meta += `<dt>Link:</dt>
             <dd><a href="${proj.link}" target="_blank" rel="noopener">${proj.link}</a></dd>`;
        }
        meta += "</dl>";

        let html = `<h1>${proj.title}</h1>${meta}`;
        if (proj.description) {
            html += `
      <section class="project-abstract">
        <h2>Description</h2>
        ${proj.description}
      </section>`;
        }
        headerEl.innerHTML = html;

        // — Find related publications
        const related = citations.filter(c => Array.isArray(c.project) && c.project.includes(projId));
        if (!related.length) return;
        relatedEl.style.display = "block";

        const base = "/";
        related.forEach(cit => {
            const div = document.createElement("div");
            div.className = "citation-container";
            // note: we escape < and > in bibtex so no stray <p> appears
            const safeBib = cit.bibtex
                ? cit.bibtex.replace(/</g,"&lt;").replace(/>/g,"&gt;")
                : "";
            div.innerHTML = `
              <div class="citation">
                <a href="${cit.link}" class="citation-image" aria-label="${cit.title}">
                  <img src="${base}${cit.image}" alt="${cit.title}" loading="lazy">
                </a>
                <div class="citation-text">
                  <a href="${cit.link}" class="citation-title">${cit.title}</a>
                  <div class="citation-authors">${cit.authors.join(", ")}</div>
                  <div class="citation-details">
                    <span class="citation-publisher">${cit.publisher}</span>
                    &nbsp;·&nbsp;
                    <span class="citation-date">
                      ${new Date(cit.date).toLocaleDateString(undefined,{
                        day:"2-digit",month:"short",year:"numeric"
                    })}
                    </span>
                    &nbsp;·&nbsp;
                    <span class="citation-doi">${cit.doi}</span>
                  </div>
                  ${cit.description
                        ? `<div class="citation-description">${cit.description}</div>`
                        : ""
                    }
                  ${safeBib
                        ? `<button class="show-bibtex" data-bibtex="${safeBib}">
                         BibTeX
                       </button>`
                        : ""
                    }
                </div>
              </div>`;
            listEl.appendChild(div);
        });
    });

    // -- BibTeX modal wiring (exactly like on your other page) --
    document.addEventListener("click", e => {
        const modal = document.getElementById("globalBibtexModal");
        const content = document.getElementById("globalBibtexContent");

        // open
        if (e.target.matches(".show-bibtex")) {
            content.textContent = e.target.getAttribute("data-bibtex");
            modal.style.display = "block";
        }
        // close
        if (e.target.matches(".close")) {
            modal.style.display = "none";
        }
        // copy
        if (e.target.matches(".copy-bibtex")) {
            const txt = document.getElementById(e.target.dataset.target).textContent;
            navigator.clipboard.writeText(txt);
        }
    });

    // close on outside click
    window.addEventListener("click", e => {
        const modal = document.getElementById("globalBibtexModal");
        if (e.target === modal) modal.style.display = "none";
    });
</script>

<link rel="stylesheet" href="/assets/css/project.css">
  </section>


    </main>
    


<footer class="background" style="--image: url('/images/background.jpg')" data-dark="true" data-size="wide">

  <div>
    © 2025
    Vision and Immersive Realities Lab
      |   Built with
    <a href="https://github.com/greenelab/lab-website-template">
      Lab Website Template
    </a>
  </div>

  <input type="checkbox" class="dark-toggle" data-tooltip="Dark mode" aria-label="toggle dark mode" oninput="onDarkToggleChange(event)">
</footer>

  </body>
</html>
