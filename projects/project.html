<!DOCTYPE html>
<html lang="en" data-dark="false">
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!--
  put your analytics (e.g. Google Analytics) tracking code here
-->

  <!--
  put your search engine verification (e.g. Google Search Console) tag here
-->

  


























<meta name="viewport" content="width=device-width, initial-scale=1">

<title>Vision and Immersive Realities Lab</title>

<link rel="icon" href="/images/icon.png">

<meta name="title" content="">
<meta name="description" content="CS@UTSA. An easy-to-use, flexible website template for labs, with automatic citations, GitHub tag imports, pre-built components, and more.">

<meta property="og:title" content="">
<meta property="og:site_title" content="Vision and Immersive Realities Lab">
<meta property="og:description" content="CS@UTSA. An easy-to-use, flexible website template for labs, with automatic citations, GitHub tag imports, pre-built components, and more.">
<meta property="og:url" content="https://utsa-virlab.github.io">
<meta property="og:image" content="/images/share.jpg">
<meta property="og:locale" content="en_US">

<meta property="twitter:title" content="">
<meta property="twitter:description" content="CS@UTSA. An easy-to-use, flexible website template for labs, with automatic citations, GitHub tag imports, pre-built components, and more.">
<meta property="twitter:url" content="https://utsa-virlab.github.io">
<meta property="twitter:card" content="summary_large_image">
<meta property="twitter:image" content="/images/share.jpg">


  <meta property="og:type" content="website">


<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "WebSite",
    
    "name": "",
    "description": "CS@UTSA. An easy-to-use, flexible website template for labs, with automatic citations, GitHub tag imports, pre-built components, and more.",
    "headline": "",
    "publisher": {
      "@type": "Organization",
      "logo": { "@type": "ImageObject", "url": "/images/icon.png" }
    },
    "url": "https://utsa-virlab.github.io"
  }
</script>

<link rel="alternate" type="application/rss+xml" href="https://utsa-virlab.github.io/feed.xml">

  <!-- Google Fonts -->
<!-- automatically get url from fonts used in theme file -->

<link rel="preconnect" href="https://fonts.gstatic.com">
<link href="https://fonts.googleapis.com/css2?display=swap&&family=Barlow:ital,wght@0,200;0,400;0,500;0,600;1,200;1,400;1,500;1,600&amp;family=Roboto+Mono:ital,wght@0,200;0,400;0,500;0,600;1,200;1,400;1,500;1,600" rel="stylesheet">

<!-- Font Awesome icons (load asynchronously due to size) -->

<link href="https://use.fontawesome.com/releases/v6.5.0/css/all.css" rel="stylesheet" media="none" onload="this.removeAttribute('media'); this.onload = null;">
<noscript>
  <link href="https://use.fontawesome.com/releases/v6.5.0/css/all.css" rel="stylesheet">
</noscript>

  <!-- third party styles -->
<!-- https://stylishthemes.github.io/Syntax-Themes/pygments/ -->
<link href="https://cdn.jsdelivr.net/gh/StylishThemes/Syntax-Themes/pygments/css-github/pygments-tomorrow-night-eighties.css" rel="stylesheet">

<!-- include all sass in styles folder -->


  
    <link href="/_styles/-theme.css" rel="stylesheet">
  

  
    <link href="/_styles/alert.css" rel="stylesheet">
  

  
    <link href="/_styles/all.css" rel="stylesheet">
  

  
    <link href="/_styles/anchor.css" rel="stylesheet">
  

  
    <link href="/_styles/background.css" rel="stylesheet">
  

  
    <link href="/_styles/body.css" rel="stylesheet">
  

  
    <link href="/_styles/bold.css" rel="stylesheet">
  

  
    <link href="/_styles/button.css" rel="stylesheet">
  

  
    <link href="/_styles/card.css" rel="stylesheet">
  

  
    <link href="/_styles/checkbox.css" rel="stylesheet">
  

  
    <link href="/_styles/citation.css" rel="stylesheet">
  

  
    <link href="/_styles/code.css" rel="stylesheet">
  

  
    <link href="/_styles/cols.css" rel="stylesheet">
  

  
    <link href="/_styles/dark-toggle.css" rel="stylesheet">
  

  
    <link href="/_styles/details.css" rel="stylesheet">
  

  
    <link href="/_styles/feature.css" rel="stylesheet">
  

  
    <link href="/_styles/figure.css" rel="stylesheet">
  

  
    <link href="/_styles/float.css" rel="stylesheet">
  

  
    <link href="/_styles/font.css" rel="stylesheet">
  

  
    <link href="/_styles/footer.css" rel="stylesheet">
  

  
    <link href="/_styles/form.css" rel="stylesheet">
  

  
    <link href="/_styles/grid.css" rel="stylesheet">
  

  
    <link href="/_styles/header.css" rel="stylesheet">
  

  
    <link href="/_styles/heading.css" rel="stylesheet">
  

  
    <link href="/_styles/highlight.css" rel="stylesheet">
  

  
    <link href="/_styles/home.css" rel="stylesheet">
  

  
    <link href="/_styles/icon.css" rel="stylesheet">
  

  
    <link href="/_styles/image.css" rel="stylesheet">
  

  
    <link href="/_styles/link.css" rel="stylesheet">
  

  
    <link href="/_styles/list.css" rel="stylesheet">
  

  
    <link href="/_styles/main.css" rel="stylesheet">
  

  
    <link href="/_styles/member.css" rel="stylesheet">
  

  
    <link href="/_styles/paragraph.css" rel="stylesheet">
  

  
    <link href="/_styles/portrait.css" rel="stylesheet">
  

  
    <link href="/_styles/post-excerpt.css" rel="stylesheet">
  

  
    <link href="/_styles/post-info.css" rel="stylesheet">
  

  
    <link href="/_styles/post-nav.css" rel="stylesheet">
  

  
    <link href="/_styles/quote.css" rel="stylesheet">
  

  
    <link href="/_styles/rule.css" rel="stylesheet">
  

  
    <link href="/_styles/search-box.css" rel="stylesheet">
  

  
    <link href="/_styles/search-info.css" rel="stylesheet">
  

  
    <link href="/_styles/section.css" rel="stylesheet">
  

  
    <link href="/_styles/sponsors.css" rel="stylesheet">
  

  
    <link href="/_styles/table.css" rel="stylesheet">
  

  
    <link href="/_styles/tags.css" rel="stylesheet">
  

  
    <link href="/_styles/textbox.css" rel="stylesheet">
  

  
    <link href="/_styles/tooltip.css" rel="stylesheet">
  

  
    <link href="/_styles/util.css" rel="stylesheet">
  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  


<!-- include all css in styles folder -->



  <!-- third party scripts -->
<script src="https://unpkg.com/@popperjs/core@2" defer></script>
<script src="https://unpkg.com/tippy.js@6" defer></script>
<script src="https://unpkg.com/mark.js@8" defer></script>

<!-- include all js in scripts folder -->


  <script src="/_scripts/anchors.js"></script>

  <script src="/_scripts/dark-mode.js"></script>

  <script src="/_scripts/fetch-tags.js"></script>

  <script src="/_scripts/home.js"></script>

  <script src="/_scripts/projects.js"></script>

  <script src="/_scripts/publications.js"></script>

  <script src="/_scripts/search.js"></script>

  <script src="/_scripts/site-search.js"></script>

  <script src="/_scripts/table-wrap.js"></script>

  <script src="/_scripts/tooltip.js"></script>


</head>

  <body>
    







<header class="background" style="--image: url('/images/background.jpg')" data-dark="true">
  <a href="/" class="home">
    
      <span class="logo">
        
          <img src="/images/logo.png" alt="logo">
        
      </span>
    
    
      <span class="title-text" data-tooltip="Home">
        
          <span class="title">Vision and Immersive Realities Lab</span>
        
        
          <span class="subtitle">CS@UTSA</span>
        
      </span>
    
  </a>

  <input class="nav-toggle" type="checkbox" aria-label="show/hide nav">

  <nav>
    
    
      
        <a href="/" data-tooltip="Back to the lab’s homepage">
          Home
        </a>
      
    
      
        <a href="/projects/" data-tooltip="Software, datasets, and more">
          Projects
        </a>
      
    
      
        <a href="/research/" data-tooltip="Published works">
          Publications
        </a>
      
    
      
        <a href="/team/" data-tooltip="About our team">
          People
        </a>
      
    
      
        <a href="/sponsors/" data-tooltip="Sponsors &amp; Partners">
          Sponsors
        </a>
      
    
  </nav>
</header>

    <main>
      <!--
  modify main content of page:
  - add section breaks
  - attach section properties
  - filter out blank sections
-->






  
  
  

  <section class="background" data-size="page">
    <div class="project-detail"></div>

<script id="projects-data" type="application/json">
    [{"title":"Identifying Bias in STEM Workplaces Towards Persons with Disabilities","id":"seed-bias","agency":"UTSA VPR Office - SRA RIG","award":"$20,000","investigator":"John Quarles, Kathy Ewoldt, Kevin Desai","role":"Co-PI (33%)","timeline":"12/2024 - 07/2025","group":"current","image":"images/photo.jpg","description":null,"tags":["resource"]},{"title":"HCC: Medium: Adaptive Auditory Feedback to Improve Balance in Virtual Reality at Home","id":"nsf-medium-audio","agency":"NSF - (CISE) Core Programs - Medium","award":"$1,200,000","investigator":"John Quarles, Kevin Desai, Alberto Cordova","role":"Co-PI (33%)","timeline":"10/2024 - 09/2027","group":"current","image":"images/photo.jpg","link":"https://www.nsf.gov/awardsearch/showAward?AWD_ID=2403411","description":null,"tags":["resource"]},{"title":"Exploring Online Learning in VR-Supported STEM Laboratories","id":"seed-chem-labs","agency":"UTSA VPR Office - UTSA-ITESM Seed Funding Program","award":"$40,000","investigator":"Kevin Desai, John Quarles","role":"Co-PI (50%)","timeline":"06/2024 - 05/2025","group":"current","image":"images/photo.jpg","link":"https://www.utsa.edu/today/2024/09/story/utsa-tec-de-monterrey-tap-virtual-reality-tools.html","description":null,"tags":["resource"]},{"title":"HCC: Small: Making Virtual Reality Safe","id":"nsf-small-safety","agency":"NSF - (CISE) Core Programs","award":"$600,000","investigator":"John Quarles, Kevin Desai","role":"Co-PI (50%)","timeline":"01/2024 - 12/2026","group":"current","image":"images/photo.jpg","link":"https://www.nsf.gov/awardsearch/showAward?AWD_ID=2316240","description":"Although consumer-level virtual reality head-mounted displays have become affordable enough for a broad user base to purchase, there are still serious concerns about safety. Head-mounted displays block out the surrounding real world, which can hide obstacles, such as tables, pets, walls, or other potential collision hazards. Current approaches to avoiding collisions depend on the user to define play area boundaries; this process is subject to user error and thus can lead to injury. Moreover, current approaches are ineffective for games that require fast motions, as the systems may not react in time to prevent injury. To address these problems, the investigators will create camera-based methods for detecting potential collisions in real time and evaluate feedback techniques to reduce the incidence of injury when using virtual reality headsets. This approach has the potential to make the use of virtual reality much safer in real-world environments. The objective of this project is to create and evaluate reconstruction, segmentation, and motion-prediction techniques to inform obstacle avoidance feedback and reduce the incidence of injury to people using virtual reality head mounted displays. Providing feedback tailored to the specific locations of the user's body that are in proximity to real obstacles will, ideally, reduce user collisions with real-world objects compared to prior approaches. Specifically, the team will 1) determine the best approaches to real obstacle detection and segmentation, 2) investigate the efficacy of full body motion prediction approaches, 3) ascertain the optimal modality and locations of real obstacle alerts to maximize presence while minimizing collisions, and 4) evaluate the longitudinal impact of real obstacle alert systems in virtual reality systems. Ultimately, this research will result in human motion datasets that can be used for future motion prediction in virtual reality research, as well as open-source plugins that will make current virtual reality experiences safer by reducing injuries.","tags":["resource"]},{"title":"HCI in Motion -- Using EEG, Eye Tracking, and Body Sensing for Attention-Aware Mobile Mixed Reality","id":"nsf-medium-motion","agency":"NSF - (CISE) Core Programs","award":"$457,105","investigator":"John Quarles, Kevin Desai","role":"Co-PI (50%)","timeline":"09/2022 - 08/2025","group":"current","image":"images/photo.jpg","link":"https://www.nsf.gov/awardsearch/showAward?AWD_ID=2211785","description":"Mobile, wireless, headsets for virtual and augmented reality, such as the Meta Quest 2 and Microsoft HoloLens-2, are becoming more widely used in many applications beyond video games, such as training, construction, and medicine. However, wearing these head-worn goggles while walking can make some people feel sick or distracted, which has even led to injury in some cases. This effect is similar to texting while walking, but potentially worse because a person's entire periphery can be filled with distracting media elements. While previous research has investigated these issues when users are standing still or seated, it is unclear how problems unfold and how they can be prevented while users are in motion. Specifically, this project will investigate how and why virtual and augmented reality headsets affect attention and feelings of sickness. First, this work will record data, such as heart rate, brain waves, and the direction users are turning their eyes to, while they are wearing virtual and augmented reality headsets and walking. Secondly, this project will develop ways to reduce sickness and distraction while walking with virtual and augmented reality headsets. This work will improve the safety of mobile virtual and augmented reality headsets, products that virtually all big technology companies today heavily invest in as possible companions or replacements to smartphones. This project will be introduced in courses and research mentorship projects at The University of Texas at San Antonio and the University of California at Santa Barbara, to advance research training of both undergraduate and graduate students. Considering that both universities and research teams have a history of supporting many underrepresented minority students, it is expected that the educational value of this project will be high, especially in terms of recruiting and mentoring women and underrepresented minority students. There is an increasing prevalence of mobile, immersive interfaces (e.g., mobile Virtual Reality(VR) / Augmented Reality (AR)) that may affect users' cognitive capacities and situational awareness, potentially leading to physical harm (e.g., impaired task performance, tripping over physical obstacles in VR, unsafe street crossings while seeing advertisements in AR). The landscape of human-computer interaction has expanded from fairly well standardized stationary office configurations to more varied mobile and immersive settings involving active body movements (mobile and situated computing, AR, mobile VR) and simulated first-person perspective changes and motion experiences (immersive computing). To make matters worse, compared to more standardized platforms such as desktop and laptop UIs, tablet and smartphone interfaces, individual differences among users have a much bigger usability impact in context-driven surround-focus usage scenarios found in mobile AR/VR. For example, motion sickness (i.e., cybersickness) in VR is known to inflict symptoms of widely varying severity, depending on the individual user. One serious consequence is that interaction designers have difficulties providing engaging general experiences that are universally usable by a wide variety of users. Despite the increasing prevalence of immersive technologies and their pitfalls, the precise cognitive and physiological mechanisms at play when 'computing in motion' are not well understood. This work is aimed at filling this knowledge gap. The specific objectives are: 1) to assess the cognitive effects of interacting with mobile AR/VR while users are walking, 2) to provide automated tools to effectively reduce the cognitive demand of mobile AR/VR, and 3) to make mobile AR/VR safer and more usable. Based on preliminary data, the central hypothesis is that through multi-modal sensing combined with machine learning approaches, mobile AR/VR applications can learn the characteristics of user behavior and provide real time adaptations that will reduce user error, increase ease of use, improve task performance, and reduce the impact of physical hazards. This work will improve the safety of mobile AR and mobile VR, paradigms that virtually all big technology companies today heavily invest in as a possible follow-up paradigm to the smartphone platform. Educational impact will occur through incorporation of research outcomes into undergraduate and graduate courses offered at The University of Texas at San Antonio and the University of California at Santa Barbara, and research training and mentorship opportunities for both undergraduate and graduate students. The courses include Machine Learning, Deep Learning for Visual Computing, Human-Computer Interaction, and Mobile Application Programming. Because our project integrates a topic of high social impact with cutting edge machine learning and human-computer interaction research along with proven successful mentorship strategies, the educational impacts of the project will be high, especially in terms of recruiting and mentoring women and underrepresented minority students.","repo":"greenelab/lab-website-template","tags":["resource"]},{"title":"CRII: HCC: 3D Hand & Full-Body Pose Estimation in Telehealth for Children with Autism","id":"nsf-crii","agency":"NSF - (CISE) Research Initiation Initiative (CRII)","award":"$174,368","investigator":"Kevin Desai","role":"PI (100%)","timeline":"06/2022 - 12/2025","group":"current","image":"images/photo.jpg","link":"https://www.nsf.gov/awardsearch/showAward?AWD_ID=2153249","description":"This award is funded in whole or in part under the American Rescue Plan Act of 2021 (Public Law 117-2). The overall objective of this project is to provide efficient full-body interaction in virtual reality systems that do not use head-mounted displays. This project aims to create accurate and real-time 3D hand and body pose estimation, in the highly significant application area of children with autism. A novel synthetic hand data generation framework will generate 3D hand poses with increased diversity in terms of hand distance from camera, hand size, camera viewpoint, occlusion, background, and skin color. The outcome will be a novel 3D synthetic hand dataset consisting of realistic and kinematically accurate hand models with articulated poses that will advance current and future research endeavors in 3D hand pose estimation research. The project will advance the state-of-the-art in 3D body pose estimation for humans present further away from the camera at room-scale distances. The synthetic dataset, algorithms, and programming libraries will be made publicly available for wide-spread adoption, thereby advancing pose estimation research. This research will have broad societal impact because it will improve the usability and interaction in human centered telehealth applications, initially helping with the applied behavior analysis for children with autism. Existing systems that employ head-mounted displays or wearable sensors for tracking the user's hand and body movements are not suitable for children with autism, and have disadvantages in many other application areas. Therefore, by enabling 3D hand and full-body pose estimation, this project will advance a plethora of 3D immersive applications such as education, virtual STEM laboratories, tele-rehabilitation, tele-operation, military training, entertainment, and communication. The need for real-time, remote and interactive human motion sensing exists now more than ever, considering the increase in virtual activities because of the recent pandemic.","repo":"greenelab/lab-website-template","tags":["resource"]},{"title":"LIGHT-SEAL: Hardware Trojan Aware Photonic Transformer for Secured Generative AI","id":"seed-photonic","agency":"UTSA SDS Collaborative Seed Funding Grant","award":"$35,000","investigator":"Dharnidhar Dang, Kevin Desai","role":"Co-PI (50%)","timeline":"07/2024 - 12/2024","group":"past","image":"images/photo.jpg","link":"https://www.linkedin.com/posts/sarah-esquivel_the-collaborative-seed-funding-grant-program-activity-7250228237501399043-wmzM/?utm_source=share&utm_medium=member_desktop","description":null,"tags":["resource"]},{"title":"Towards Personalized Virtual Reality Interventions for Rehabilitation of Persons with Disabilities","id":"seed-mac","agency":"UTSA VPR Office - MAC-UTSA Seed Funding Program","award":"$25,000","investigator":"John Quarles, Kevin Desai, Alberto Cordova","role":"Co-PI (33%)","timeline":"10/2023 - 06/2024","group":"past","image":"images/photo.jpg","link":"https://www.utsa.edu/today/2024/06/story/UTSA-morgans-MAC-showcase-impactful-disability-research.html","description":null,"tags":["resource"]},{"title":"Electronic Health Record Big Data and Radiomic Analytics for Precision Medicine Approach to Long-COVID","id":"sappt","subtitle":"a subtitle","agency":"San Antonio Partnership for Precision Therapeutics","award":"$50,000","investigator":"Dhireesha Kudithipudi, Kevin Desai, Anandi Dutta","role":"Co-PI (25%)","timeline":"08/2022 - 02/2024","group":"past","image":"images/photo.jpg","description":"This award is funded in whole or in part under the American Rescue Plan Act of 2021 (Public Law 117-2). The overall objective of this project is to provide efficient full-body interaction in virtual reality systems that do not use head-mounted displays. This project aims to create accurate and real-time 3D hand and body pose estimation, in the highly significant application area of children with autism. A novel synthetic hand data generation framework will generate 3D hand poses with increased diversity in terms of hand distance from camera, hand size, camera viewpoint, occlusion, background, and skin color. The outcome will be a novel 3D synthetic hand dataset consisting of realistic and kinematically accurate hand models with articulated poses that will advance current and future research endeavors in 3D hand pose estimation research. The project will advance the state-of-the-art in 3D body pose estimation for humans present further away from the camera at room-scale distances. The synthetic dataset, algorithms, and programming libraries will be made publicly available for wide-spread adoption, thereby advancing pose estimation research. This research will have broad societal impact because it will improve the usability and interaction in human centered telehealth applications, initially helping with the applied behavior analysis for children with autism. Existing systems that employ head-mounted displays or wearable sensors for tracking the user's hand and body movements are not suitable for children with autism, and have disadvantages in many other application areas. Therefore, by enabling 3D hand and full-body pose estimation, this project will advance a plethora of 3D immersive applications such as education, virtual STEM laboratories, tele-rehabilitation, tele-operation, military training, entertainment, and communication. The need for real-time, remote and interactive human motion sensing exists now more than ever, considering the increase in virtual activities because of the recent pandemic.","repo":"greenelab/lab-website-template","tags":["resource"]},{"title":"A step towards smart and connected health in behavior analysis","id":"seed-great-sleep","agency":"UTSA VPR Office - GREAT 2021","award":"$20,000","investigator":"Leslie Neely, Paul Rad, Qian Chen, Kevin Desai","role":"Co-PI (25%)","timeline":"10/2021 - 07/2022","group":"past","image":"images/photo.jpg","link":"https://www.utsa.edu/today/2021/08/story/knowledge-enterprise-funds-innovative-new-research-projects.html#:~:text=A%20step%20towards%20smart%20and%20connected%20health%20in%20behavior%20analysis","repo":"greenelab/lab-website-template","tags":["software"]},{"title":"Project Lovelace 2.0: Advancing Women in AI Career Pathways","id":"xilinx","agency":"Xilinx Inc. WIT University Grants 2021","award":"$30,000","investigator":"Dhireesha Kudithipudi, Amina Qutub, Kevin Desai","role":"Co-PI (33%)","timeline":"09/2021 - 08/2022","group":"past","image":"images/photo.jpg","link":"https://ai.utsa.edu/xilinxfellows/","repo":"greenelab/lab-website-template","tags":["past"]},{"title":"Broadening Participation in Computer Science","id":"","agency":"Northeastern University - Center for Inclusive Computing","award":"$499,801","role":"Co-PI (10%)","investigator":"Jianwei Niu, John Heaps, Amanda Fernandez, Mitra Bokaei Hosseini, Kevin Desai, Rocky Slavin","timeline":"01/2024 - 12/2025","group":"current","image":"images/photo.jpg","link":"https://sciences.utsa.edu/spotlight-news/2024/grant-to-prepare-students-for-computer-science-careers.html","description":null,"tags":["resource"]},{"title":"MATCH: MATRIX AI/ML Concierge for Healthcare","id":"","agency":"NIH (UNT Passthrough)","award":"$500,000","role":"Co-PI (10%)","investigator":"Dhireesha Kudithipudi, Amina Qutub, Mark Goldberg, Ambika Mathur, Kevin Desai, Panagiotis Markopoulos, Erica Sosa","timeline":"10/2024 - 09/2025","group":"current","image":"images/photo.jpg","link":"https://ai.utsa.edu/mpower/","description":null,"tags":["resource"]},{"title":"M-POWER: MATRIX-Provided AI/ML Open-Source Resource Center for Behavioral Health EmpoWERment","id":"","agency":"NIH (UNT Passthrough)","award":"$500,000","role":"Co-PI (10%)","investigator":"Dhireesha Kudithipudi, Amina Qutub, Mark Goldberg, Ambika Mathur, Kevin Desai, Panagiotis Markopoulos, Anandi Dutta, Erica Sosa","timeline":"10/2023 - 07/2024","group":"past","image":"images/photo.jpg","link":"https://ai.utsa.edu/mpower/","description":null,"tags":["resource"]}]
</script>
<script id="citations-data" type="application/json">
    [{"id":"DOI: 10.1109/ISM.2015.111","title":"Network adaptive textured mesh generation for collaborative 3d tele-immersion","project":["nsf-small-safety","nsf-medium-motion"],"authors":["Kevin Desai","Kanchan Bahirat","Suraj Raghuraman","Balakrishnan Prabhakaran"],"publisher":"2015 IEEE International Symposium on Multimedia (ISM)","date":"2015-12-14","link":"https://ieeexplore.ieee.org/abstract/document/7442307","orcid":"0000-0002-4655-3773","plugin":"orcid.py","file":"orcid.yaml","image":"images/no-preview.png","bibtex":"@inproceedings{desai2015network,organization={IEEE},year={2015},pages={107--112},booktitle={2015 IEEE International Symposium on Multimedia(ISM)},author={Desai,Kevin and Bahirat,Kanchan and Raghuraman,Suraj and Prabhakaran,Balakrishnan},title={Network adaptive textured mesh generation for collaborative 3d tele-immersion}}"},{"id":"DOI: 10.1145/2910017.2910612","title":"Augmented reality-based exergames for rehabilitation","project":["nsf-small-safety","nsf-medium-motion"],"authors":["Kevin Desai","Kanchan Bahirat","Sudhir Ramalingam","Balakrishnan Prabhakaran","Thiru Annaswamy","Una E Makris"],"publisher":"Proceedings of the 7th International Conference on Multimedia Systems","date":"2016-05-10","link":"https://dl.acm.org/doi/abs/10.1145/2910017.2910612","orcid":"0000-0002-4655-3773","plugin":"orcid.py","file":"orcid.yaml","image":"images/no-preview.png","bibtex":"@inproceedings{desai2016augmented,year={2016},pages={1--10},booktitle={Proceedings of the 7th International Conference on Multimedia Systems},author={Desai,Kevin and Bahirat,Kanchan and Ramalingam,Sudhir and Prabhakaran,Balakrishnan and Annaswamy,Thiru and Makris,Una E},title={Augmented reality-based exergames for rehabilitation}}"},{"id":"DOI: 10.1109/BigMM.2017.62","title":"Experiences with multi-modal collaborative virtual laboratory (mmcvl)","authors":["Kevin Desai","Uriel Haile Hernndez Belmonte","Rong Jin","Balakrishnan Prabhakaran","Paul Diehl","Victor Ayala Ramirez","Vinu Johnson","Murry Gans"],"publisher":"2017 IEEE third international conference on multimedia big data (BigMM)","date":"2017-04-19","link":"https://ieeexplore.ieee.org/abstract/document/7966775","orcid":"0000-0002-4655-3773","plugin":"orcid.py","file":"orcid.yaml","image":"images/no-preview.png","bibtex":"@inproceedings{desai2017experiences,organization={IEEE},year={2017},pages={376--383},booktitle={2017 IEEE third international conference on multimedia big data(BigMM)},author={Desai,Kevin and Belmonte,Uriel Haile Hernndez and Jin,Rong and Prabhakaran,Balakrishnan and Diehl,Paul and Ramirez,Victor Ayala and Johnson,Vinu and Gans,Murry},title={Experiences with multi-modal collaborative virtual laboratory(mmcvl)}}"},{"id":"DOI: 10.1109/ICME.2017.8019470","title":"Learning-based objective evaluation of 3D human open meshes","authors":["Kevin Desai","Kanchan Bahirat","Balakrishnan Prabhakaran"],"publisher":"2017 IEEE International Conference on Multimedia and Expo (ICME)","date":"2017-12-31","link":"https://ieeexplore.ieee.org/abstract/document/8019470","orcid":"0000-0002-4655-3773","plugin":"orcid.py","file":"orcid.yaml","image":"images/no-preview.png","bibtex":"@inproceedings{desai2017learning,organization={IEEE},year={2017},pages={733--738},booktitle={2017 IEEE International Conference on Multimedia and Expo(ICME)},author={Desai,Kevin and Bahirat,Kanchan and Prabhakaran,Balakrishnan},title={Learning-based objective evaluation of 3D human open meshes}}"},{"id":"DOI: 10.1109/ISM.2017.27","title":"QoE studies on interactive 3D tele-immersion","authors":["Kevin Desai","Suraj Raghuraman","Rong Jin","Balakrishnan Prabhakaran"],"publisher":"2017 IEEE international symposium on multimedia (ISM)","date":"2017-12-11","link":"https://ieeexplore.ieee.org/abstract/document/8241591","orcid":"0000-0002-4655-3773","plugin":"orcid.py","file":"orcid.yaml","image":"images/no-preview.png","bibtex":"@inproceedings{desai2017qoe,organization={IEEE},year={2017},pages={130--137},booktitle={2017 IEEE international symposium on multimedia(ISM)},author={Desai,Kevin and Raghuraman,Suraj and Jin,Rong and Prabhakaran,Balakrishnan},title={QoE studies on interactive 3D tele-immersion}}"},{"id":"DOI: 10.1145/3204949.3204969","title":"Skeleton-based continuous extrinsic calibration of multiple RGB-D kinect cameras","authors":["Kevin Desai","Balakrishnan Prabhakaran","Suraj Raghuraman"],"publisher":"Proceedings of the 9th ACM multimedia systems conference","date":"2018-06-12","link":"https://dl.acm.org/doi/abs/10.1145/3204949.3204969","orcid":"0000-0002-4655-3773","plugin":"orcid.py","file":"orcid.yaml","image":"images/no-preview.png","bibtex":"@inproceedings{desai2018skeleton,year={2018},pages={250--257},booktitle={Proceedings of the 9th ACM multimedia systems conference},author={Desai,Kevin and Prabhakaran,Balakrishnan and Raghuraman,Suraj},title={Skeleton-based continuous extrinsic calibration of multiple RGB-D kinect cameras}}"},{"id":"DOI: 10.1145/3204949.3204958","title":"Combining skeletal poses for 3D human model generation using multiple Kinects","authors":["Kevin Desai","Balakrishnan Prabhakaran","Suraj Raghuraman"],"publisher":"Proceedings of the 9th ACM multimedia systems conference","date":"2018-06-12","link":"https://dl.acm.org/doi/abs/10.1145/3204949.3204958","orcid":"0000-0002-4655-3773","plugin":"orcid.py","file":"orcid.yaml","image":"images/no-preview.png","bibtex":"@inproceedings{desai2018combining,year={2018},pages={40--51},booktitle={Proceedings of the 9th ACM multimedia systems conference},author={Desai,Kevin and Prabhakaran,Balakrishnan and Raghuraman,Suraj},title={Combining skeletal poses for 3D human model generation using multiple Kinects}}"},{"id":"DOI: 10.1145/3343031.3351165","title":"Using Mr. MAPP for lower limb phantom pain management","authors":["Kanchan Bahirat","Yu-Yen Chung","Thiru Annaswamy","Gargi Raval","Kevin Desai","Balakrishnan Prabhakaran","Michael Riegler"],"publisher":"Proceedings of the 27th ACM International Conference on Multimedia","date":"2019-10-15","link":"https://dl.acm.org/doi/abs/10.1145/3343031.3351165","orcid":"0000-0002-4655-3773","plugin":"orcid.py","file":"orcid.yaml","image":"images/no-preview.png","bibtex":"@inproceedings{bahirat2019using,year={2019},pages={1071--1075},booktitle={Proceedings of the 27th ACM International Conference on Multimedia},author={Bahirat,Kanchan and Chung,Yu-Yen and Annaswamy,Thiru and Raval,Gargi and Desai,Kevin and Prabhakaran,Balakrishnan and Riegler,Michael},title={Using Mr. MAPP for lower limb phantom pain management}}"},{"id":"DOI: 10.1080/17483107.2021.1913518","title":"Personalized 3D exergames for in-home rehabilitation after stroke: a pilot study","authors":["Kevin Desai","Balakrishnan Prabhakaran","Nneka Ifejika","Thiru M Annaswamy"],"publisher":"Disability and Rehabilitation: Assistive Technology","date":"2021-04-28","link":"https://www.tandfonline.com/doi/full/10.1080/17483107.2021.1913518","orcid":"0000-0002-4655-3773","plugin":"orcid.py","file":"orcid.yaml","image":"images/no-preview.png","bibtex":"@article{desai2021personalized,publisher={Taylor & Francis},year={2021},pages={1--10},journal={Disability and Rehabilitation: Assistive Technology},author={Desai,Kevin and Prabhakaran,Balakrishnan and Ifejika,Nneka and Annaswamy,Thiru M},title={Personalized 3D exergames for in-home rehabilitation after stroke: a pilot study}}"},{"id":"DOI: 10.1109/SMC52423.2021.9659223","title":"Show why the answer is correct! towards explainable ai using compositional temporal attention","authors":["Nihar Bendre","Kevin Desai","Peyman Najafirad"],"publisher":"2021 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","date":"2021-10-17","link":"https://ieeexplore.ieee.org/abstract/document/9659223","orcid":"0000-0002-4655-3773","plugin":"orcid.py","file":"orcid.yaml","image":"images/no-preview.png","bibtex":"@inproceedings{bendre2021show,organization={IEEE},year={2021},pages={3006--3012},booktitle={2021 IEEE International Conference on Systems,Man,and Cybernetics(SMC)},author={Bendre,Nihar and Desai,Kevin and Najafirad,Peyman},title={Show why the answer is correct! towards explainable ai using compositional temporal attention}}"},{"id":"DOI: 10.1109/ICIP42928.2021.9506108","title":"Generalized zero-shot learning using multimodal variational auto-encoder with semantic concepts","authors":["Nihar Bendre","Kevin Desai","Peyman Najafirad"],"publisher":"2021 IEEE International Conference on Image Processing (ICIP)","date":"2021-09-19","link":"https://ieeexplore.ieee.org/abstract/document/9506108","orcid":"0000-0002-4655-3773","plugin":"orcid.py","file":"orcid.yaml","image":"images/no-preview.png","bibtex":"@inproceedings{bendre2021generalized,organization={IEEE},year={2021},pages={1284--1288},booktitle={2021 IEEE International Conference on Image Processing(ICIP)},author={Bendre,Nihar and Desai,Kevin and Najafirad,Peyman},title={Generalized zero-shot learning using multimodal variational auto-encoder with semantic concepts}}"},{"id":"DOI: 10.48550/arXiv.2108.06437","title":"VR Sickness Prediction from Integrated HMD's Sensors using Multimodal Deep Fusion Network","authors":["Rifatul Islam","Kevin Desai","John Quarles"],"publisher":"arXiv preprint arXiv:2108.06437","date":"2021-08-14","link":"https://arxiv.org/abs/2108.06437","orcid":"0000-0002-4655-3773","plugin":"orcid.py","file":"orcid.yaml","image":"images/no-preview.png","bibtex":"@article{islam2021vr,year={2021},journal={arXiv preprint arXiv:2108.06437},author={Islam,Rifatul and Desai,Kevin and Quarles,John},title={VR Sickness Prediction from Integrated HMD's Sensors using Multimodal Deep Fusion Network}}"},{"id":"DOI: 10.1109/ISMAR52148.2021.00017","title":"Cybersickness prediction from integrated hmd’s sensors: A multimodal deep fusion approach using eye-tracking and head-tracking data","authors":["Rifatul Islam","Kevin Desai","John Quarles"],"publisher":"2021 IEEE international symposium on mixed and augmented reality (ISMAR)","date":"2021-10-04","link":"https://ieeexplore.ieee.org/abstract/document/9583838","orcid":"0000-0002-4655-3773","plugin":"orcid.py","file":"orcid.yaml","image":"images/no-preview.png","bibtex":"@inproceedings{islam2021cybersickness,organization={IEEE},year={2021},pages={31--40},booktitle={2021 IEEE international symposium on mixed and augmented reality(ISMAR)},author={Islam,Rifatul and Desai,Kevin and Quarles,John},title={Cybersickness prediction from integrated hmd’s sensors: A multimodal deep fusion approach using eye-tracking and head-tracking data}}"},{"id":"DOI: 10.23919/WAC50355.2021.9559516","title":"Multimodal data streaming using visual IoTs and wearables for telerehabilitation and teletreatment","authors":["Herbert Guzman","Reenam Joshi","Victor Guzman","Max Kilger","Kevin Desai"],"publisher":"2021 World Automation Congress (WAC)","date":"2021-08-01","link":"https://ieeexplore.ieee.org/abstract/document/9559516","orcid":"0000-0002-4655-3773","plugin":"orcid.py","file":"orcid.yaml","image":"images/no-preview.png","bibtex":"@inproceedings{guzman2021multimodal,organization={IEEE},year={2021},pages={233--238},booktitle={2021 World Automation Congress(WAC)},author={Guzman,Herbert and Joshi,Reenam and Guzman,Victor and Kilger,Max and Desai,Kevin},title={Multimodal data streaming using visual IoTs and wearables for telerehabilitation and teletreatment}}"},{"id":"DOI: 10.1109/CVPRW56347.2022.00286","title":"Hr-stan: High-resolution spatio-temporal attention network for 3d human motion prediction","authors":["Omar Medjaouri","Kevin Desai"],"publisher":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","date":"2022-08-23","link":"https://openaccess.thecvf.com/content/CVPR2022W/Precognition/html/Medjaouri_HR-STAN_High-Resolution_Spatio-Temporal_Attention_Network_for_3D_Human_Motion_Prediction_CVPRW_2022_paper.html","orcid":"0000-0002-4655-3773","plugin":"orcid.py","file":"orcid.yaml","image":"images/no-preview.png","bibtex":"@inproceedings{medjaouri2022hr,year={2022},pages={2540--2549},booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},author={Medjaouri,Omar and Desai,Kevin},title={Hr-stan: High-resolution spatio-temporal attention network for 3d human motion prediction}}"},{"id":"DOI: 10.1109/CVPRW56347.2022.00036","title":"Tmvnet: Using transformers for multi-view voxel-based 3d reconstruction","authors":["Kebin Peng","Rifatul Islam","John Quarles","Kevin Desai"],"publisher":"Proceedings of the IEEE/CVF conference on computer vision and pattern recognition","date":"2022-08-23","link":"https://openaccess.thecvf.com/content/CVPR2022W/PBVS/html/Peng_TMVNet_Using_Transformers_for_Multi-View_Voxel-Based_3D_Reconstruction_CVPRW_2022_paper.html","orcid":"0000-0002-4655-3773","plugin":"orcid.py","file":"orcid.yaml","image":"images/no-preview.png","bibtex":"@inproceedings{peng2022tmvnet,year={2022},pages={222--230},booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},author={Peng,Kebin and Islam,Rifatul and Quarles,John and Desai,Kevin},title={Tmvnet: Using transformers for multi-view voxel-based 3d reconstruction}}"},{"id":"DOI: 10.5220/0010879800003124","title":"BRDF-based Irradiance Image Estimation to Remove Radiometric Differences for Stereo Matching","authors":["Kebin Peng","John Quarles","Kevin Desai"],"publisher":"VISIGRAPP (5: VISAPP)","date":"2022-06-01","link":"https://www.scitepress.org/PublicationsDetail.aspx?ID=Df1h/LGTeUQ=&t=1","orcid":"0000-0002-4655-3773","plugin":"orcid.py","file":"orcid.yaml","image":"images/no-preview.png","bibtex":"@inproceedings{peng2022brdf,year={2022},pages={734--744},booktitle={VISIGRAPP(5: VISAPP)},author={Peng,Kebin and Quarles,John and Desai,Kevin},title={BRDF-based Irradiance Image Estimation to Remove Radiometric Differences for Stereo Matching.}}"},{"id":"DOI: 10.1145/3532106.3533486","title":"Virtepex: virtual remote tele-physical examination system","authors":["Ninad Khargonkar","Kevin Desai","Balakrishnan Prabhakaran","Thiru Annaswamy"],"publisher":"Proceedings of the 2022 ACM Designing Interactive Systems Conference","date":"2022-06-13","link":"https://dl.acm.org/doi/abs/10.1145/3532106.3533486","orcid":"0000-0002-4655-3773","plugin":"orcid.py","file":"orcid.yaml","image":"images/no-preview.png","bibtex":"@inproceedings{khargonkar2022virtepex,year={2022},pages={1729--1742},booktitle={Proceedings of the 2022 ACM Designing Interactive Systems Conference},author={Khargonkar,Ninad and Desai,Kevin and Prabhakaran,Balakrishnan and Annaswamy,Thiru},title={Virtepex: virtual remote tele-physical examination system}}"},{"id":"DOI: 10.1109/ISMAR55827.2022.00026","title":"Towards forecasting the onset of cybersickness by fusing physiological, head-tracking and eye-tracking with multimodal deep fusion network","authors":["Rifatul Islam","Kevin Desai","John Quarles"],"publisher":"2022 IEEE international symposium on mixed and augmented reality (ISMAR)","date":"2022-10-17","link":"https://ieeexplore.ieee.org/abstract/document/9995267","orcid":"0000-0002-4655-3773","plugin":"orcid.py","file":"orcid.yaml","image":"images/no-preview.png","bibtex":"@inproceedings{islam2022towards,organization={IEEE},year={2022},pages={121--130},booktitle={2022 IEEE international symposium on mixed and augmented reality(ISMAR)},author={Islam,Rifatul and Desai,Kevin and Quarles,John},title={Towards forecasting the onset of cybersickness by fusing physiological,head-tracking and eye-tracking with multimodal deep fusion network}}"},{"id":"DOI: 10.1109/ICPR56361.2022.9956096","title":"PMPNet: Pixel Movement Prediction Network for Monocular Depth Estimation in Dynamic Scenes","authors":["Kebin Peng","John Quarles","Kevin Desai"],"publisher":"2022 26th International Conference on Pattern Recognition (ICPR)","date":"2022-08-21","link":"https://ieeexplore.ieee.org/abstract/document/9956096","orcid":"0000-0002-4655-3773","plugin":"orcid.py","file":"orcid.yaml","image":"images/no-preview.png","bibtex":"@inproceedings{peng2022pmpnet,organization={IEEE},year={2022},pages={3915--3921},booktitle={2022 26th International Conference on Pattern Recognition(ICPR)},author={Peng,Kebin and Quarles,John and Desai,Kevin},title={PMPNet: Pixel Movement Prediction Network for Monocular Depth Estimation in Dynamic Scenes}}"},{"id":"DOI: 10.1007/s41252-022-00309-y","title":"The case for integrated advanced technology in applied behavior analysis","authors":["Leslie Neely","Amarie Carnett","John Quarles","Hannah MacNaul","Se-Woong Park","Sakiko Oyama","Guenevere Chen","Kevin Desai","Peyman Najafirad"],"publisher":"Advances in Neurodevelopmental Disorders","date":"2023-09-01","link":"https://link.springer.com/article/10.1007/s41252-022-00309-y","orcid":"0000-0002-4655-3773","plugin":"orcid.py","file":"orcid.yaml","image":"images/no-preview.png","bibtex":"@article{neely2023case,publisher={Springer International Publishing Cham},year={2023},pages={415--425},number={3},volume={7},journal={Advances in Neurodevelopmental Disorders},author={Neely,Leslie and Carnett,Amarie and Quarles,John and MacNaul,Hannah and Park,Se-Woong and Oyama,Sakiko and Chen,Guenevere and Desai,Kevin and Najafirad,Peyman},title={The case for integrated advanced technology in applied behavior analysis}}"},{"id":"DOI: 10.3390/s23020929","title":"Can Hierarchical Transformers Learn Facial Geometry?","authors":["Paul Young","Nima Ebadi","Arun Das","Mazal Bethany","Kevin Desai","Peyman Najafirad"],"publisher":"Sensors","date":"2023-01-13","link":"https://www.mdpi.com/1424-8220/23/2/929","orcid":"0000-0002-4655-3773","plugin":"orcid.py","file":"orcid.yaml","image":"images/no-preview.png","bibtex":"@article{young2023can,publisher={MDPI},year={2023},pages={929},number={2},volume={23},journal={Sensors},author={Young,Paul and Ebadi,Nima and Das,Arun and Bethany,Mazal and Desai,Kevin and Najafirad,Peyman},title={Can Hierarchical Transformers Learn Facial Geometry?}}"},{"id":"DOI: 10.48550/arXiv.2302.05795","title":"Assessment HTN (A-HTN) for Automated Task Performance Assessment in 3D Serious Games","authors":["Kevin Desai","Omeed Ashtiani","Balakrishnan Prabhakaran"],"publisher":"arXiv preprint arXiv:2302.05795","date":"2023-02-11","link":"https://arxiv.org/abs/2302.05795","orcid":"0000-0002-4655-3773","plugin":"orcid.py","file":"orcid.yaml","image":"images/no-preview.png","bibtex":"@article{desai2023assessment,year={2023},journal={arXiv preprint arXiv:2302.05795},author={Desai,Kevin and Ashtiani,Omeed and Prabhakaran,Balakrishnan},title={Assessment HTN(A-HTN)for Automated Task Performance Assessment in 3D Serious Games}}"},{"id":"DOI: 10.1109/ICIT58056.2023.10225860","title":"A Study of Human Fitness Pose Classification Using Artificial Neural Networks","authors":["Sijie Shang","Rong Jin","Kevin Desai"],"publisher":"2023 International Conference on Information Technology (ICIT)","date":"2023-08-09","link":"https://ieeexplore.ieee.org/abstract/document/10225860","orcid":"0000-0002-4655-3773","plugin":"orcid.py","file":"orcid.yaml","image":"images/no-preview.png","bibtex":"@inproceedings{shang2023study,organization={IEEE},year={2023},pages={250--255},booktitle={2023 International Conference on Information Technology(ICIT)},author={Shang,Sijie and Jin,Rong and Desai,Kevin},title={A Study of Human Fitness Pose Classification Using Artificial Neural Networks}}"},{"id":"DOI: 10.1109/TVCG.2024.3372122","title":"Investigating personalization techniques for improved cybersickness prediction in virtual reality environments","authors":["Umama Tasnim","Rifatul Islam","Kevin Desai","John Quarles"],"publisher":"IEEE Transactions on Visualization and Computer Graphics","date":"2024-03-04","link":"https://ieeexplore.ieee.org/abstract/document/10458344","orcid":"0000-0002-4655-3773","plugin":"orcid.py","file":"orcid.yaml","image":"images/no-preview.png","bibtex":"@article{tasnim2024investigating,publisher={IEEE},year={2024},journal={IEEE Transactions on Visualization and Computer Graphics},author={Tasnim,Umama and Islam,Rifatul and Desai,Kevin and Quarles,John},title={Investigating personalization techniques for improved cybersickness prediction in virtual reality environments}}"},{"id":"DOI: 10.1109/IPDPSW63119.2024.00117","title":"A converting autoencoder toward low-latency and energy-efficient DNN inference at the edge","authors":["Hasanul Mahmud","Peng Kang","Kevin Desai","Palden Lama","Sushil K Prasad"],"publisher":"2024 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)","date":"2024-05-27","link":"https://ieeexplore.ieee.org/abstract/document/10596358","orcid":"0000-0002-4655-3773","plugin":"orcid.py","file":"orcid.yaml","image":"images/no-preview.png","bibtex":"@inproceedings{mahmud2024converting,organization={IEEE},year={2024},pages={592--599},booktitle={2024 IEEE International Parallel and Distributed Processing Symposium Workshops(IPDPSW)},author={Mahmud,Hasanul and Kang,Peng and Desai,Kevin and Lama,Palden and Prasad,Sushil K},title={A converting autoencoder toward low-latency and energy-efficient DNN inference at the edge}}"},{"id":"DOI: 10.48550/arXiv.2403.17893","title":"A survey on 3d egocentric human pose estimation","authors":["Md Mushfiqur Azam","Kevin Desai"],"publisher":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","date":"2024-01-01","link":"https://openaccess.thecvf.com/content/CVPR2024W/Rhobin/html/Azam_A_Survey_on_3D_Egocentric_Human_Pose_Estimation_CVPRW_2024_paper.html","orcid":"0000-0002-4655-3773","plugin":"orcid.py","file":"orcid.yaml","image":"images/no-preview.png","bibtex":"@inproceedings{azam2024survey,year={2024},pages={1643--1654},booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},author={Azam,Md Mushfiqur and Desai,Kevin},title={A survey on 3d egocentric human pose estimation}}"},{"id":"DOI: 10.48550/arXiv.2404.13770","title":"EncodeNet: A Framework for Boosting DNN Accuracy with Entropy-Driven Generalized Converting Autoencoder","authors":["Hasanul Mahmud","Palden Lama","Kevin Desai","Sushil K Prasad"],"publisher":"International Conference on Pattern Recognition","date":"2024-12-01","link":"https://link.springer.com/chapter/10.1007/978-3-031-78166-7_30","orcid":"0000-0002-4655-3773","plugin":"orcid.py","file":"orcid.yaml","image":"images/no-preview.png","bibtex":"@inproceedings{mahmud2024encodenet,organization={Springer Nature Switzerland Cham},year={2024},pages={463--477},booktitle={International Conference on Pattern Recognition},author={Mahmud,Hasanul and Lama,Palden and Desai,Kevin and Prasad,Sushil K},title={EncodeNet: A Framework for Boosting DNN Accuracy with Entropy-Driven Generalized Converting Autoencoder}}"},{"id":"DOI: 10.1109/Cloud-Summit61220.2024.00028","title":"CAE-Net: Enhanced Converting Autoencoder Based Framework for Low-Latency Energy-Efficient DNN with SLO-Constraints","authors":["Hasanul Mahmud","Peng Kang","Palden Lama","Kevin Desai","Sushil K Prasad"],"publisher":"2024 IEEE Cloud Summit","date":"2024-06-27","link":"https://ieeexplore.ieee.org/abstract/document/10631035","orcid":"0000-0002-4655-3773","plugin":"orcid.py","file":"orcid.yaml","image":"images/no-preview.png","bibtex":"@inproceedings{mahmud2024cae,organization={IEEE},year={2024},pages={128--134},booktitle={2024 IEEE Cloud Summit},author={Mahmud,Hasanul and Kang,Peng and Lama,Palden and Desai,Kevin and Prasad,Sushil K},title={CAE-Net: Enhanced Converting Autoencoder Based Framework for Low-Latency Energy-Efficient DNN with SLO-Constraints}}"},{"id":"DOI: 10.1109/ISMAR62088.2024.00121","title":"Mazed and Confused: A Dataset of Cybersickness, Working Memory, Mental Load, Physical Load, and Attention During a Real Walking Task in VR","authors":["Jyotirmay Nag Setu","Joshua M Le","Ripan Kumar Kundu","Barry Giesbrecht","Tobias Höllerer","Khaza Anuarul Hoque","Kevin Desai","John Quarles"],"publisher":"2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)","date":"2024-10-21","link":"https://ieeexplore.ieee.org/abstract/document/10765425","orcid":"0000-0002-4655-3773","plugin":"orcid.py","file":"orcid.yaml","image":"images/no-preview.png","bibtex":"@inproceedings{setu2024mazed,organization={IEEE},year={2024},pages={1048--1057},booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality(ISMAR)},author={Setu,Jyotirmay Nag and Le,Joshua M and Kundu,Ripan Kumar and Giesbrecht,Barry and H{\"o}llerer,Tobias and Hoque,Khaza Anuarul and Desai,Kevin and Quarles,John},title={Mazed and Confused: A Dataset of Cybersickness,Working Memory,Mental Load,Physical Load,and Attention During a Real Walking Task in VR}}"},{"id":"DOI: 10.2196/57443","title":"Comparing In-Person, Standard Telehealth, and Remote Musculoskeletal Examination With a Novel Augmented Reality Exercise Game System: Pilot Cross-Sectional Comparison Study","authors":["Richard Wu","Keerthana Chakka","Sara Belko","Ninad Khargonkar","Kevin Desai","Balakrishnan Prabhakaran","Thiru Annaswamy","others"],"publisher":"JMIR Serious Games","date":"2025-02-05","link":"https://games.jmir.org/2025/1/e57443/","orcid":"0000-0002-4655-3773","plugin":"orcid.py","file":"orcid.yaml","image":"images/no-preview.png","bibtex":"@article{wu2025comparing,publisher={JMIR Publications Inc.,Toronto,Canada},year={2025},pages={e57443},number={1},volume={13},journal={JMIR Serious Games},author={Wu,Richard and Chakka,Keerthana and Belko,Sara and Khargonkar,Ninad and Desai,Kevin and Prabhakaran,Balakrishnan and Annaswamy,Thiru and others},title={Comparing In-Person,Standard Telehealth,and Remote Musculoskeletal Examination With a Novel Augmented Reality Exercise Game System: Pilot Cross-Sectional Comparison Study}}"},{"id":"mubashshira2025te","title":"TE-NeRF: Triplane-Enhanced Neural Radiance Field for Artifact-Free Human Rendering","authors":["Sadia Mubashshira","Kevin Desai"],"publisher":"Proceedings of the Winter Conference on Applications of Computer Vision","date":"2025-01-01","link":"https://openaccess.thecvf.com/content/WACV2025W/ImageQuality/html/Mubashshira_TE-NeRF_Triplane-Enhanced_Neural_Radiance_Field_for_Artifact-Free_Human_Rendering_WACVW_2025_paper.html","orcid":"0000-0002-4655-3773","plugin":"orcid.py","file":"orcid.yaml","image":"images/no-preview.png","bibtex":"@inproceedings{mubashshira2025te,year={2025},pages={238--247},booktitle={Proceedings of the Winter Conference on Applications of Computer Vision},author={Mubashshira,Sadia and Desai,Kevin},title={TE-NeRF: Triplane-Enhanced Neural Radiance Field for Artifact-Free Human Rendering}}"},{"id":"eghbalian2025applying","title":"Applying Computer Vision to Analyze Self-Injurious Behaviors in Children with Autism Spectrum Disorder","authors":["Ayda Eghbalian","Md Mushfiqur Azam","Katie Holloway","Leslie Neely","Kevin Desai"],"publisher":"Proceedings of the Winter Conference on Applications of Computer Vision","date":"2025-01-01","link":"https://openaccess.thecvf.com/content/WACV2025W/CV4Small/html/Eghbalian_Applying_Computer_Vision_to_Analyze_Self-Injurious_Behaviors_in_Children_with_WACVW_2025_paper.html","orcid":"0000-0002-4655-3773","plugin":"orcid.py","file":"orcid.yaml","image":"images/no-preview.png","bibtex":"@inproceedings{eghbalian2025applying,year={2025},pages={11--20},booktitle={Proceedings of the Winter Conference on Applications of Computer Vision},author={Eghbalian,Ayda and Azam,Md Mushfiqur and Holloway,Katie and Neely,Leslie and Desai,Kevin},title={Applying Computer Vision to Analyze Self-Injurious Behaviors in Children with Autism Spectrum Disorder}}"},{"id":"DOI: 10.1109/TVCG.2025.3549850","title":"Predicting and Explaining Cognitive Load, Attention, and Working Memory in Virtual Multitasking","authors":["Jyotirmay Nag Setu","Joshua M Le","Ripan Kumar Kundu","Barry Giesbrecht","Tobias Höllerer","Khaza Anuarul Hoque","Kevin Desai","John Quarles"],"publisher":"IEEE Transactions on Visualization and Computer Graphics","date":"2025-03-10","link":"https://ieeexplore.ieee.org/abstract/document/10919242","orcid":"0000-0002-4655-3773","plugin":"orcid.py","file":"orcid.yaml","image":"images/no-preview.png","bibtex":"@article{setu2025predicting,publisher={IEEE},year={2025},journal={IEEE Transactions on Visualization and Computer Graphics},author={Setu,Jyotirmay Nag and Le,Joshua M and Kundu,Ripan Kumar and Giesbrecht,Barry and H{\"o}llerer,Tobias and Hoque,Khaza Anuarul and Desai,Kevin and Quarles,John},title={Predicting and Explaining Cognitive Load,Attention,and Working Memory in Virtual Multitasking}}"}]
</script>

<script>
    document.addEventListener("DOMContentLoaded", () => {
        // parse
        let projects = JSON.parse(document.getElementById("projects-data").textContent);
        let citations = JSON.parse(document.getElementById("citations-data").textContent);

        // lookup
        const params = new URLSearchParams(window.location.search);
        const id     = params.get("id");
        const proj   = projects.find(p => p.id === id);
        const container = document.querySelector(".project-detail");
        if (!proj) return container.innerHTML = "<p><em>Project not found.</em></p>";

        // build <dl>
        let dl = "<dl class='project-meta'>";
        ["agency","award","investigator","role","timeline"].forEach(key => {
            let label = key.charAt(0).toUpperCase()+key.slice(1);
            dl += `<dt>${label}:</dt><dd>${proj[key]||"&mdash;"}</dd>`;
        });
        // **NEW**: Link entry
        if (proj.link) {
            dl += `<dt>Link:</dt>
           <dd><a href="${proj.link}" target="_blank" rel="noopener">
             ${proj.link}
           </a></dd>`;
        }
        dl += "</dl>";

        // assemble
        let html = `<h1>${proj.title}</h1>${dl}`;

        if (proj.description) {
            html += `
      <section class="project-abstract">
        <h2>Abstract</h2>
        ${proj.description}
      </section>`;
        }

        // related pubs
        const related = citations.filter(c => Array.isArray(c.project) && c.project.includes(id));
        if (related.length) {
            html += `
      <section class="project-section">
        <h2>Related Publications</h2>
        <ul class="pub-list">`;
            related.forEach(pub => {
                const year = new Date(pub.date).getFullYear();
                html += `
          <li>
            <a href="${pub.link}" target="_blank" rel="noopener">
              ${pub.title}
            </a>
            <br><small>${pub.publisher} (${year})</small>
          </li>`;
            });
            html += `
        </ul>
      </section>`;
        }

        container.innerHTML = html;
    });
</script>

<style>
    .project-detail {
        max-width: 1000px;
        margin: 2rem auto;
        text-align: justify;
        padding: 0 1rem;
    }
    .project-detail h1 {
        margin-bottom: 1rem;
    }
    .project-meta {
        display: grid;
        grid-template-columns: max-content auto;
        column-gap: 1rem;
        row-gap: .5rem;
        justify-content: center;
        margin-bottom: 2rem;
    }
    .project-meta dt,
    .project-meta dd {
        margin: 0;
        padding: 0;
    }
    .project-abstract,
    .project-section {
        margin-top: 2rem;
        line-height: 1.6;
    }
    .project-abstract h2,
    .project-section h2 {
        margin-bottom: .5rem;
    }
    .pub-list {
        list-style: disc inside;
        margin: 0;
        padding: 0;
    }
    .pub-list li {
        margin-bottom: .75rem;
    }
    .pub-list a {
        font-weight: bold;
    }
    .pub-list small {
        color: #555;
    }
</style>
  </section>


    </main>
    


<footer class="background" style="--image: url('/images/background.jpg')" data-dark="true" data-size="wide">
  <!--
    <div>
      Extra details like contact info or address
    </div>
  -->

  <div>
    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="mailto:contact@your-lab.com" data-tooltip="Email" data-style="bare" aria-label="Email">
      <i class="icon fa-solid fa-envelope"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://orcid.org/0000-0001-8713-9213" data-tooltip="ORCID" data-style="bare" aria-label="ORCID">
      <i class="icon fa-brands fa-orcid"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://scholar.google.com/citations?user=ZzaTdlYAAAAJ&hl" data-tooltip="Google Scholar" data-style="bare" aria-label="Google Scholar">
      <i class="icon fa-brands fa-google"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://github.com/your-lab" data-tooltip="GitHub" data-style="bare" aria-label="GitHub">
      <i class="icon fa-brands fa-github"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://twitter.com/YourLabHandle" data-tooltip="Twitter" data-style="bare" aria-label="Twitter">
      <i class="icon fa-brands fa-twitter"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://youtube.com/YourLabChannel" data-tooltip="YouTube" data-style="bare" aria-label="YouTube">
      <i class="icon fa-brands fa-youtube"></i>
      
    </a>
  </div>


    
  </div>

  <div>
    © 2025
    Vision and Immersive Realities Lab
      |   Built with
    <a href="https://github.com/greenelab/lab-website-template">
      Lab Website Template
    </a>
  </div>

  <input type="checkbox" class="dark-toggle" data-tooltip="Dark mode" aria-label="toggle dark mode" oninput="onDarkToggleChange(event)">
</footer>

  </body>
</html>
