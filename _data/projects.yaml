- title: "Identifying Bias in STEM Workplaces Towards Persons with Disabilities"
  id: "seed-bias"
  agency: "UTSA VPR Office - SRA RIG"
  award: "$20,000"
  investigator: "John Quarles, Kathy Ewoldt, Kevin Desai"
  role: "Co-PI (33%)"
  timeline: "12/2024 - 07/2025"
  group: current
  image: images/photo.jpg
  description:
  tags:
    - research

- title: "HCC: Medium: Adaptive Auditory Feedback to Improve Balance in Virtual Reality at Home"
  id: "nsf-medium-audio"
  agency: "NSF - (CISE) Core Programs - Medium"
  award: "$1,200,000"
  investigator: "John Quarles, Kevin Desai, Alberto Cordova"
  role: "Co-PI (33%)"
  timeline: "10/2024 - 09/2027"
  group: current
  image: images/photo.jpg
  link: https://www.nsf.gov/awardsearch/showAward?AWD_ID=2403411
  description: "Consumer-level virtual reality goggles are not accessible for many persons with balance impairments, such as elderly persons, persons with multiple sclerosis, Parkinson's, or diabetes. Currently, people with balance impairments cannot benefit from many immersive virtual reality benefits, such as education, physical fitness, rehabilitation, and entertainment. The team's previous work studied how audio, visuals, and vibrations can improve balance while in virtual reality in controlled laboratory settings with simplified virtual reality environments, where they found that audio was the most effective. This project aims to expand the research beyond laboratory settings because typical use of virtual reality occurs at home with complex virtual reality environments with much more intense audio, visual, and vibration stimulation. To address the additional complexities of being outside the lab with much stronger stimulation, the team will develop specialized audio intended to improve balance in any virtual environment. If virtual reality imbalance issues can be resolved, persons with balance impairments can more readily benefit from virtual reality. This project investigates approaches to enable adaptive auditory feedback techniques to improve balance in virtual reality use at home, which includes commercial virtual reality experiences with strong stimuli. Based on the team's preliminary studies, the central hypothesis is that adaptive auditory feedback can improve balance during virtual reality use at home more effectively than the current state of the art, which uses a static, 'one size fits all' approach to feedback. The work will seek the following novel contributions: 1) adaptive auditory feedback techniques to improve balance in VR; 2) automatic, real-time balance prediction with low-cost sensors, some of which are already integrated into commercial virtual reality systems; 3) controlled laboratory and at-home studies, providing generalizable understanding and computational models of balance in virtual reality in the presence of strong stimuli; 4) insight into the potentially lasting effects of virtual reality-based auditory feedback on balance after virtual reality exposure. Ultimately, this project will result in datasets and open-source tools that will make virtual reality more accessible for persons with balance impairments, which could improve their quality of life."
  tags:
    - research

- title: "Exploring Online Learning in VR-Supported STEM Laboratories"
  id: "seed-chem-labs"
  agency: "UTSA VPR Office - UTSA-ITESM Seed Funding Program"
  award: "$40,000"
  investigator: "Kevin Desai, John Quarles"
  role: "Co-PI (50%)"
  timeline: "06/2024 - 08/2025"
  group: current
  image: images/photo.jpg
  link: https://www.utsa.edu/today/2024/09/story/utsa-tec-de-monterrey-tap-virtual-reality-tools.html
  description:
  tags:
    - research

- title: "HCC: Small: Making Virtual Reality Safe"
  id: "nsf-small-safety"
  agency: "NSF - (CISE) Core Programs"
  award: "$600,000"
  investigator: "John Quarles, Kevin Desai"
  role: "Co-PI (50%)"
  timeline: "01/2024 - 12/2026"
  group: current
  image: images/photo.jpg
  link: https://www.nsf.gov/awardsearch/showAward?AWD_ID=2316240
  description: Although consumer-level virtual reality head-mounted displays have become affordable enough for a broad user base to purchase, there are still serious concerns about safety. Head-mounted displays block out the surrounding real world, which can hide obstacles, such as tables, pets, walls, or other potential collision hazards. Current approaches to avoiding collisions depend on the user to define play area boundaries; this process is subject to user error and thus can lead to injury. Moreover, current approaches are ineffective for games that require fast motions, as the systems may not react in time to prevent injury. To address these problems, the investigators will create camera-based methods for detecting potential collisions in real time and evaluate feedback techniques to reduce the incidence of injury when using virtual reality headsets. This approach has the potential to make the use of virtual reality much safer in real-world environments. The objective of this project is to create and evaluate reconstruction, segmentation, and motion-prediction techniques to inform obstacle avoidance feedback and reduce the incidence of injury to people using virtual reality head mounted displays. Providing feedback tailored to the specific locations of the user's body that are in proximity to real obstacles will, ideally, reduce user collisions with real-world objects compared to prior approaches. Specifically, the team will 1) determine the best approaches to real obstacle detection and segmentation, 2) investigate the efficacy of full body motion prediction approaches, 3) ascertain the optimal modality and locations of real obstacle alerts to maximize presence while minimizing collisions, and 4) evaluate the longitudinal impact of real obstacle alert systems in virtual reality systems. Ultimately, this research will result in human motion datasets that can be used for future motion prediction in virtual reality research, as well as open-source plugins that will make current virtual reality experiences safer by reducing injuries.
  tags:
    - research

- title: "HCI in Motion -- Using EEG, Eye Tracking, and Body Sensing for Attention-Aware Mobile Mixed Reality"
  id: "nsf-medium-motion"
  agency: "NSF - (CISE) Core Programs"
  award: "$457,105"
  investigator: "John Quarles, Kevin Desai"
  role: "Co-PI (50%)"
  timeline: "09/2022 - 08/2025"
  group: current
  image: images/photo.jpg
  link: https://www.nsf.gov/awardsearch/showAward?AWD_ID=2211785
  description: "Mobile, wireless, headsets for virtual and augmented reality, such as the Meta Quest 2 and Microsoft HoloLens-2, are becoming more widely used in many applications beyond video games, such as training, construction, and medicine. However, wearing these head-worn goggles while walking can make some people feel sick or distracted, which has even led to injury in some cases. This effect is similar to texting while walking, but potentially worse because a person's entire periphery can be filled with distracting media elements. While previous research has investigated these issues when users are standing still or seated, it is unclear how problems unfold and how they can be prevented while users are in motion. Specifically, this project will investigate how and why virtual and augmented reality headsets affect attention and feelings of sickness. First, this work will record data, such as heart rate, brain waves, and the direction users are turning their eyes to, while they are wearing virtual and augmented reality headsets and walking. Secondly, this project will develop ways to reduce sickness and distraction while walking with virtual and augmented reality headsets. This work will improve the safety of mobile virtual and augmented reality headsets, products that virtually all big technology companies today heavily invest in as possible companions or replacements to smartphones. This project will be introduced in courses and research mentorship projects at The University of Texas at San Antonio and the University of California at Santa Barbara, to advance research training of both undergraduate and graduate students. Considering that both universities and research teams have a history of supporting many underrepresented minority students, it is expected that the educational value of this project will be high, especially in terms of recruiting and mentoring women and underrepresented minority students. There is an increasing prevalence of mobile, immersive interfaces (e.g., mobile Virtual Reality(VR) / Augmented Reality (AR)) that may affect users' cognitive capacities and situational awareness, potentially leading to physical harm (e.g., impaired task performance, tripping over physical obstacles in VR, unsafe street crossings while seeing advertisements in AR). The landscape of human-computer interaction has expanded from fairly well standardized stationary office configurations to more varied mobile and immersive settings involving active body movements (mobile and situated computing, AR, mobile VR) and simulated first-person perspective changes and motion experiences (immersive computing). To make matters worse, compared to more standardized platforms such as desktop and laptop UIs, tablet and smartphone interfaces, individual differences among users have a much bigger usability impact in context-driven surround-focus usage scenarios found in mobile AR/VR. For example, motion sickness (i.e., cybersickness) in VR is known to inflict symptoms of widely varying severity, depending on the individual user. One serious consequence is that interaction designers have difficulties providing engaging general experiences that are universally usable by a wide variety of users. Despite the increasing prevalence of immersive technologies and their pitfalls, the precise cognitive and physiological mechanisms at play when 'computing in motion' are not well understood. This work is aimed at filling this knowledge gap. The specific objectives are: 1) to assess the cognitive effects of interacting with mobile AR/VR while users are walking, 2) to provide automated tools to effectively reduce the cognitive demand of mobile AR/VR, and 3) to make mobile AR/VR safer and more usable. Based on preliminary data, the central hypothesis is that through multi-modal sensing combined with machine learning approaches, mobile AR/VR applications can learn the characteristics of user behavior and provide real time adaptations that will reduce user error, increase ease of use, improve task performance, and reduce the impact of physical hazards. This work will improve the safety of mobile AR and mobile VR, paradigms that virtually all big technology companies today heavily invest in as a possible follow-up paradigm to the smartphone platform. Educational impact will occur through incorporation of research outcomes into undergraduate and graduate courses offered at The University of Texas at San Antonio and the University of California at Santa Barbara, and research training and mentorship opportunities for both undergraduate and graduate students. The courses include Machine Learning, Deep Learning for Visual Computing, Human-Computer Interaction, and Mobile Application Programming. Because our project integrates a topic of high social impact with cutting edge machine learning and human-computer interaction research along with proven successful mentorship strategies, the educational impacts of the project will be high, especially in terms of recruiting and mentoring women and underrepresented minority students."
  tags:
    - research

- title: "CRII: HCC: 3D Hand & Full-Body Pose Estimation in Telehealth for Children with Autism"
  id: "nsf-crii"
  agency: "NSF - (CISE) Research Initiation Initiative (CRII)"
  award: "$174,368"
  investigator: "Kevin Desai"
  role: "PI (100%)"
  timeline: "06/2022 - 12/2025"
  group: current
  image: images/photo.jpg
  link: https://www.nsf.gov/awardsearch/showAward?AWD_ID=2153249
  description: "The overall objective of this project is to provide efficient full-body interaction in virtual reality systems that do not use head-mounted displays. This project aims to create accurate and real-time 3D hand and body pose estimation, in the highly significant application area of children with autism. A novel synthetic hand data generation framework will generate 3D hand poses with increased diversity in terms of hand distance from camera, hand size, camera viewpoint, occlusion, background, and skin color. The outcome will be a novel 3D synthetic hand dataset consisting of realistic and kinematically accurate hand models with articulated poses that will advance current and future research endeavors in 3D hand pose estimation research. The project will advance the state-of-the-art in 3D body pose estimation for humans present further away from the camera at room-scale distances. The synthetic dataset, algorithms, and programming libraries will be made publicly available for wide-spread adoption, thereby advancing pose estimation research. This research will have broad societal impact because it will improve the usability and interaction in human centered telehealth applications, initially helping with the applied behavior analysis for children with autism. Existing systems that employ head-mounted displays or wearable sensors for tracking the user's hand and body movements are not suitable for children with autism, and have disadvantages in many other application areas. Therefore, by enabling 3D hand and full-body pose estimation, this project will advance a plethora of 3D immersive applications such as education, virtual STEM laboratories, tele-rehabilitation, tele-operation, military training, entertainment, and communication. The need for real-time, remote and interactive human motion sensing exists now more than ever, considering the increase in virtual activities because of the recent pandemic."
  tags:
    - research

- title: "LIGHT-SEAL: Hardware Trojan Aware Photonic Transformer for Secured Generative AI"
  id: "seed-photonic"
  agency: "UTSA SDS Collaborative Seed Funding Grant"
  award: "$35,000"
  investigator: "Dharnidhar Dang, Kevin Desai"
  role: "Co-PI (50%)"
  timeline: "07/2024 - 12/2024"
  group: past
  image: images/photo.jpg
  link: https://sds.utsa.edu//news/2024/10/seedgrant.html
  description:
  tags:
    - research

- title: "Towards Personalized Virtual Reality Interventions for Rehabilitation of Persons with Disabilities"
  id: "seed-mac"
  agency: "UTSA VPR Office - MAC-UTSA Seed Funding Program"
  award: "$25,000"
  investigator: "John Quarles, Kevin Desai, Alberto Cordova"
  role: "Co-PI (33%)"
  timeline: "10/2023 - 06/2024"
  group: past
  image: images/photo.jpg
  link: https://www.utsa.edu/today/2024/06/story/UTSA-morgans-MAC-showcase-impactful-disability-research.html
  description:
  tags:
    - research

- title: "Electronic Health Record Big Data and Radiomic Analytics for Precision Medicine Approach to Long-COVID"
  id: "sappt"
  subtitle: a subtitle
  agency: "San Antonio Partnership for Precision Therapeutics"
  award: "$50,000"
  investigator: "Dhireesha Kudithipudi, Kevin Desai, Anandi Dutta"
  role: "Co-PI (25%)"
  timeline: "08/2022 - 02/2024"
  group: past
  image: images/photo.jpg
  description: 
  tags:
    - research

- title: "A step towards smart and connected health in behavior analysis"
  id: "seed-great-sleep"
  agency: "UTSA VPR Office - GREAT 2021"
  award: "$20,000"
  investigator: "Leslie Neely, Paul Rad, Qian Chen, Kevin Desai"
  role: "Co-PI (25%)"
  timeline: "10/2021 - 07/2022"
  group: past
  image: images/photo.jpg
  link: https://www.utsa.edu/today/2021/08/story/knowledge-enterprise-funds-innovative-new-research-projects.html#:~:text=A%20step%20towards%20smart%20and%20connected%20health%20in%20behavior%20analysis
  description:
  tags:
    - research

- title: "Project Lovelace 2.0: Advancing Women in AI Career Pathways"
  id: "xilinx"
  agency: "Xilinx Inc. WIT University Grants 2021"
  award: "$30,000"
  investigator: "Dhireesha Kudithipudi, Amina Qutub, Kevin Desai"
  role: "Co-PI (33%)"
  timeline: "09/2021 - 08/2022"
  group: past
  image: images/photo.jpg
  link: https://ai.utsa.edu/xilinxfellows/
  description:
  tags:
    - instructional

- title: "Broadening Participation in Computer Science"
  id: ""
  agency: "Northeastern University - Center for Inclusive Computing"
  award: "$499,801"
  role: "Co-PI (10%)"
  investigator: "Jianwei Niu, John Heaps, Amanda Fernandez, Mitra Bokaei Hosseini, Kevin Desai, Rocky Slavin"
  timeline: "01/2024 - 12/2025"
  group: current
  image: images/photo.jpg
  link: https://sciences.utsa.edu/spotlight-news/2024/grant-to-prepare-students-for-computer-science-careers.html
  description:
  tags:
    - instructional

- title: "MATCH: MATRIX AI/ML Concierge for Healthcare"
  id: ""
  agency: "NIH (UNT Passthrough)"
  award: "$500,000"
  role: "Co-PI (10%)"
  investigator: "Dhireesha Kudithipudi, Amina Qutub, Mark Goldberg, Ambika Mathur, Kevin Desai, Panagiotis Markopoulos, Erica Sosa"
  timeline: "10/2024 - 09/2025"
  group: current
  image: images/photo.jpg
  link: https://healthyhuman.tech/
  description:
  tags:
    - service

- title: "M-POWER: MATRIX-Provided AI/ML Open-Source Resource Center for Behavioral Health EmpoWERment"
  id: ""
  agency: "NIH (UNT Passthrough)"
  award: "$500,000"
  role: "Co-PI (10%)"
  investigator: "Dhireesha Kudithipudi, Amina Qutub, Mark Goldberg, Ambika Mathur, Kevin Desai, Panagiotis Markopoulos, Anandi Dutta, Erica Sosa"
  timeline: "10/2023 - 07/2024"
  group: past
  image: images/photo.jpg
  link: https://ai.utsa.edu/mpower/
  description:
  tags:
    - service
